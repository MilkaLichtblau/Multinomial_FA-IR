\section{Algorithm}\label{sec:algorithms}
We present the multinomial \algoFAIR algorithm (\S\ref{subsec:algorithm-description}) and prove it is correct (\S\ref{subsec:algorithm-correctness}).

\subsection{Algorithm Description}\label{subsec:algorithm-description}
\note{Done}
Multinomial \algoFAIR, presented in Algorithm~\ref{alg:fair}, solves the {\sc Fair Top-$k$ Ranking} problem for multinomial protected groups and intersectional group settings.
%
As input, multinomial \algoFAIR takes 
the expected size $k$ of the ranking to be returned,
the qualifications $q_c$, 
indicator variables $g_c$ indicating if candidate $c$ is protected,
the vector of minimum target proportions $p_G$, and
the adjusted significance level $\alphaadj$.

First, the algorithm uses $q_c$ to create priority queues with up to $k$ candidates each: $P_0$ for the non-protected candidates and $P_g$ for the protected candidates of group $g$.
%
Next (line \ref{alg:fair:mtree}), the algorithm derives a ranked group fairness tree (mTree) similar to Figure~\ref{fig:mtree-asymmetric-adjusted}, i.e., for each position it computes the minimum number of protected candidates per group, given $p_G$, $k$ and $\alphaadj$.
%
Then, multinomial \algoFAIR greedily constructs a ranking subject to candidate qualifications, and minimum protected elements required.
%
Note that the right choice of a tree node on level $i$ depends on the path chosen along the tree and therefore on its concrete parent at level $i-1$. 
%
In case \texttt{mtree} branches into different possibilities to satisfy ranked group fairness (as an example see Fig.~\ref{fig:mtree-asymmetric-adjusted} for $k=4$), the algorithm chooses the branch that has a higher value for $F$, meaning it chooses the branch that has a higher probability.
%
If two branches are equally likely, which happens for $p_1 = p_2 = \ldots = p_{|G|}$, then one of them is chosen at random.
%
Given this the algorithm has to find the correct child $m_{G,i}$ for a given parent from the previous level (Line~\ref{alg:fair:childNode}).
%
If the node demands a protected candidate from group $g$ at the current position $i$, the algorithm appends the best candidate from $P_g$ to the ranking (Lines \ref{alg:fair:pstart}-\ref{alg:fair:pend}); otherwise, it appends the best candidate from $P_0 \cup P_1 \cup \ldots \cup P_{|G|}$ (Lines \ref{alg:fair:anystart}-\ref{alg:fair:anyend}).
%

\begin{algorithm}[h]
	%\caption{Algorithm \algoFAIR, finding a ranking that maximizes utility subject to in-group monotonicity and ranked group fairness constraints.}
	\caption{Algorithm \algoFAIR finds a ranking that maximizes utility subject to in-group monotonicity and ranked group fairness constraints. Checks for special cases (e.g., insufficient candidates of a class) are not included for clarity.}
	\label{alg:fair}  % But whenever possible refer to this algo. by name not number
	\small
	\AlgInput{$k \in [n]$, the size of the list to return; $\forall~c \in [n]$: $q_c$, the qualifications for candidate $c$, and $g_c$ an indicator that is $>0$ iff candidate $c$ is protected; $p_G$ with $\forall p \in p_G \in ]0,1[$, the vector of minimum proportions for each group of protected elements; $\alphaadj \in ]0,1[$, the adjusted significance for each fair representation test.}
	\AlgOutput{$\tau$ satisfying the group fairness condition with parameters $p, \sigma$, and maximizing utility.}
	%\AlgComment{compute min. protected candidates per position}
	$P_0, P_1, \ldots P_{|G|} \leftarrow$ empty priority queues with bounded capacity $k$\\
	\For{$c \leftarrow 1$ \KwTo $n$}{
		insert $c$ with value $q_c$ in priority queue $P_{g_c}$ \\
	}

	$\texttt{mtree}(i) \leftarrow \texttt{\algoComputeMTree}(k, p_G, \alphaadj)$  \label{alg:fair:mtree}\\
		
	%\AlgComment{create fair ranking}
	$(t_0, t_1, \ldots, t_{|G|}) \leftarrow (0, \ldots, 0)$ \\
	$i \leftarrow 0 $ \\
	\While{$i < k$}{
		\texttt{noCandidateAdded = True} \\
		\AlgComment{get next node in tree path}
		$m_{G, i} = [m_1(i), \ldots, m_{|G|}(i)] \leftarrow \texttt{findNextNode(mtree, i)}$ \label{alg:fair:childNode}\\
		\AlgComment{find which group needs a new candidate}
		\For{\texttt{g = 1; g} $\leq$ \texttt{|G|; g++}}{

			\If{$t_g < m_g(i)$}{\label{alg:fair:pstart}
				\AlgComment{add a protected candidate}
				$t_g \leftarrow t_g + 1$ \\ 
				$\tau[i] \leftarrow \operatorname{pop}(P_g)$ \\  
				\texttt{noCandidateAdded = False}
			}\label{alg:fair:pend}
		}
		\If{\texttt{noCandidateAdded}}{ \label{alg:fair:anystart}
			\AlgComment{no protected candidate needed: add the best available}
			$P_g \leftarrow$ \texttt{findBestCandidateQueue()} \\
			$\tau[i] \leftarrow \operatorname{pop}(P_g)$\\
			$t_g \leftarrow t_g + 1$ 
		}\label{alg:fair:anyend}
		
	}
	\Return{$\tau$}
\end{algorithm}
\vspace{-3mm}

\subsection{Algorithm Complexity}\label{subsec:algorithm-complexity}
\todo{Please review @Meike}
\todo{Maybe we put the comments on space and time complexity in the appendix...}
In this section, we will provide runtime and space complexities for all proposed algorithms. Table \ref{tbl:time} shows the asymptotic costs for each algorithm if there is no precomputed data available for the analyzed procedure.
\subsubsection{\algoMtable complexity}\label{subsubsec:construct-mtable-complexity}
\algoMtable computes the inverse binomial cdf for k positions of the ranking and stores each of the computed values. This leads to a time complexity of $\mathcal{O}(k) * \mathcal{O}(inverseBinomialCDF(p,k,\alpha))$. Note that the complexity of the inverse binomial cdf is also dependent on how accurate the computation is. The space complexity is $\mathcal{O}(k)$ if we do not store any intermediate results for future calculations.
\subsubsection{\algoRecursive complexity}\label{subsubsec:success-prob-complexity}
The algorithm \algoRecursive has time complexity $\mathcal{O}(\prod_{j=1}^{m(k)}b(j) \cdot O(\texttt{binomPDF}))$ as explained in section \ref{subsubsec:adjustment-binomial}. Beforehand we have to calculate the MTable and the blocks $b$. This adds $\mathcal{O}(k) * \mathcal{O}(inverseBinomialCDF(p,k,\alpha))$ for the MTable and $\mathcal{O}(k)$ for the blocks. Overall we get $\mathcal{O}(k) * \mathcal{O}(inverseBinomialCDF(p,k,\alpha)) + \mathcal{O}(\prod_{j=1}^{m(k)}b(j) \cdot O(\texttt{binomPDF}))$. For the sake of readability we will write $\mathcal{O}($\algoMtable$) + \mathcal{O}(\prod_{j=1}^{m(k)}b(j) \cdot O(\texttt{binomPDF}))$
The space complexity in the worst case is however $\mathcal{O}(k)$ for the number maximum number of blocks plus $\mathcal{O}(k)$ for the stored probabilities at each position.
\subsubsection{\algoBinomBinary complexity}\label{subsubsec:binom-binary-complexity}
The binary search for $\alpha_c$ takes $\mathcal{O}(\log{}\frac{k(k-1)}{2}) = \mathcal{O}(\log{}k^2)$ for the binary search. This is, because there exist at most $\frac{k(k-1)}{2}$ different valid MTables. For each step of the binary search we need $\mathcal{O}(\mathcal{O}(k) * \mathcal{O}(inverseBinomialCDF(p,k,\alpha)))$ to compute the MTable of the new pointer as well as $\mathcal{O}(\prod_{j=1}^{m(k)}b(j) \cdot O(\texttt{binomPDF}))$ for the fail probability of the MTable of the new pointer. Overall we get $\mathcal{O}(\log{}k^2) * (\mathcal{O}(\prod_{j=1}^{m(k)}b(j) \cdot O(\texttt{binomPDF})) + \mathcal{O}(\mathcal{O}(k) * \mathcal{O}(inverseBinomialCDF(p,k,\alpha))))$, which we will write as $\mathcal{O}(\log{}k^2) * (\mathcal{O}(\algoRecursive))$. The space complexity is $\mathcal{O}(k)$ since we only store the three MTables with their respective fail probability.
\subsubsection{\algoImcdf complexity}\label{subsubsec:imcdf-complexity}
Like the inverse binomial cdf and binomial pmf, the complexity of the multinomial pmf is dependant on $k,p_G , \alpha$ and the accuracy of the calculation. We will write $\mathcal{O}(MCDF(k,p_G,\alpha))$ as the asymptotic complexity of this function.
Overall we get $\mathcal{O}(|G|) * \mathcal{O}(MCDF(k,p,\alpha ))$ with $|G|$ being the number of protected groups. The space complexity is $\mathcal{O}(|G|^2)$ because we store $|G|$ nodes with an array of length $|G|$.
\subsubsection{\algoComputeMTree complexity}\label{subsubsec:mtree-complexity}
Our algorithm \algoComputeMTree , constructs an MTree of depth $k$. In the worst case, there are $|G|$ possible children for each node in the tree resulting in $|G|^{k-1} +1$ nodes. Overall we get $\mathcal{O}(|G|^{k}) * \mathcal{O}(\algoImcdf)$ for the time complexity. We store $|G|^{k-1} +1$ nodes with arrays of length $|G|$, leading to the space complexity of $\mathcal{O}(|G|^{k})$.
\subsubsection{\algoMultBinary complexity}\label{subsubsec:mtree-complexity}
As in the binomial case, we are searching for $\alpha_c$ in the multinomial case with binary search too. However, because we do not have a discrete measure for different MTrees as in the binomial case, the binary search is dependant of the tolerance $\epsilon$ set beforehand as a stop criteria. Because we are searching binary on the interval $\left[0,\alpha\right[$, the resulting number of possible MTrees is $\frac{\alpha}{\epsilon}$. Thus, the binary search needs $\mathcal{O}(\log{}\frac{\alpha}{\epsilon})$ time. For each step, we calculate one MTree and its fail probability. The fail probability is computed experimentally by creating 10.000 rankings and testing them against the MTree. This process needs $\mathcal{O}(10.000 *k)$ to create the rankings plus $\mathcal{O}(k)$ to test each of them. Overall we have $\mathcal{O}(\log{}\frac{\alpha}{\epsilon}) * (\mathcal{O}(k^2) + \mathcal{O}(\algoComputeMTree))$ for the time complexity. The space complexity is in $\mathcal{O}(|G|^k)$ for storing three MTrees and their fail probability.
\subsubsection{\algoReg complexity}\label{subsubsec:regression-complexity}
The algorithm \algoReg has the same asymptotic complexity as \algoMultBinary . However, as seen in figure \ref{fig:regression_adjustment_benefits}, there is a constant and significant reduction in computation time compared to the use of \algoMultBinary only.
\subsubsection{\algoFAIR complexity}\label{subsubsec:FAIR-complexity}
Assuming a computational cost of $\mathcal{O}(n \log{} n)$ for creating the sorted lists of candidates, \algoFAIR ranks exactly $k$ items using an adjusted MTree of height $k$. Overall we have the cost of running \algoMultBinary once and then following one path through the MTree of height $k$. Leading to $\mathcal{O}(n \log{} n) + \mathcal{O} (\algoMultBinary) + \mathcal{O}(k)$. The space complexity is that of \algoMultBinary plus $\mathcal{O}(n)$ for the candidates we want to rank + $\mathcal{O}(k)$ for the ranking itself, in summary $\mathcal{O}(|G|^k + n + k)$.

\begin{table}[H]
\caption{Time complexity for all algorithms without use of precomputed results.
		\label{tbl:time}}
\scalebox{0.75}{
\begin{tabular}{|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Time Complexity} \\ \hline
\algoMtable & $\mathcal{O}(k) * \mathcal{O}(inverseBinomialCDF(p,k,\alpha))$ \\ \hline
\algoRecursive & $\mathcal{O}(\algoMtable) + \mathcal{O}(\prod_{j=1}^{m(k)}b(j) \cdot O(\texttt{binomPDF}))$ \\ \hline
\algoBinomBinary & $\mathcal{O}(\log{}k^2) * (\mathcal{O}(\algoRecursive))$ \\ \hline
\algoImcdf & $\mathcal{O}(|G|) * \mathcal{O}(MCDF(k,p,\alpha ))$ \\ \hline
\algoComputeMTree & $\mathcal{O}(|G|^{k}) * \mathcal{O}(\algoImcdf)$ \\ \hline
\algoMultBinary & $\mathcal{O}(\log{}\frac{\alpha}{\epsilon}) * (\mathcal{O}(k^2) + \mathcal{O}(\algoComputeMTree))$ \\ \hline
\algoReg & $\mathcal{O}(\log{}\frac{\alpha}{\epsilon}) * (\mathcal{O}(k^2) + \mathcal{O}(\algoComputeMTree))$ \\ \hline
\algoFAIR & $\mathcal{O}(n \log{} n) + \mathcal{O} (\algoMultBinary) + \mathcal{O}(k)$ \\ \hline
\end{tabular}
}
\end{table}
\begin{table}[H]
\caption{Space complexity for all algorithms without use of precomputed results.
		\label{tbl:space}}
\scalebox{0.75}{
\begin{tabular}{|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Space Complexity} \\ \hline
\algoMtable & $\mathcal{O}(k)$ \\ \hline
\algoRecursive & $\mathcal{O}(k)$ \\ \hline
\algoBinomBinary & $\mathcal{O}(k)$ \\ \hline
\algoImcdf & $\mathcal{O}(|G|^2)$ \\ \hline
\algoComputeMTree & $\mathcal{O}(|G|^{k})$ \\ \hline
\algoMultBinary & $\mathcal{O}(|G|^k)$ \\ \hline
\algoReg & $\mathcal{O}(|G|^k)$ \\ \hline
\algoFAIR & $\mathcal{O}(|G|^k + n + k)$ \\ \hline
\end{tabular}
}
\end{table}
%\algoFAIR has running time $O(n + k \log k)$; which includes building the $O(k)$ size priority queues from $n$ items and processing them to obtain the final ranking, where we assume $k < O(n/\log n)$. 
%\meike{Was soll denn $k < O(n/\log n)$ heißen? Warum ist die size der priority queues O(k) und nicht k?}
%
%If we already have ranked lists for all groups of elements, \algoFAIR can avoid the first step and obtain the top-$k$ in $O(k \log k)$ time.
%
%Our method is applicable as long as there is at least one protected group and there are enough candidates in each protected group; if there are $k$ from each group, the algorithm is guaranteed to succeed, otherwise the ``head'' of the ranking will satisfy the ranked group fairness constraint, but the ``tail'' of the ranking may not.

\subsection{Algorithm Optimizations}
\note{Done}
Because the computation of an adjusted mTree is expensive (Table~\ref{tbl:space_time}), our implementation persists already computed mTrees and their components to never do the same computation twice. 
%
Depending on the structure of $p_G$, different levels of optimization are applicable.

We note however that an mTree has to be computed only once for a particular combination of $k, p_G, \alpha$ and that \algoFAIR has a complexity of $O(k log k)$.
%
We provide the already pre-computed mTrees and MCDF caches for our experiments and the intermediate steps not only for reproducibility but for use in practice too.

\subsubsection{MCDF Cache}\label{subsubsec:mcdf-cache}
Table~\ref{tbl:space_time} shows that the highest computational cost arises from computing the multinomial cumulative distribution function $F$. 
%
In the worst case Algorithm~\ref{alg:imcdf} computes it $|G|+1$ times for each group $g$ in $m_g(i)$ and each position $i\leq k$.
%
However, the same calculation may be done many times:
%
As an example consider the (fictive) mTree nodes $[2,1]$ and $[1,2]$ at position $k=3$. 
%
To compute the successors of node $[2,1]$ we call Algorithm~\ref{alg:imcdf} with arguments $(4,[2,1])$, $(4,[3,1])$ and $(4,[2,2])$. 
%
We store the results of these calculation in a map that we call MCDF cache with the algorithm arguments ($k$ and the minimum protected candidates of each group) as key and the corresponding mcdf as value.
%
Next we compute the successors of node $[1,2]$ and call Algorithm~\ref{alg:imcdf} with arguments $(4,[1,2])$, $(4,[2,2])$ and $(4,[1,3])$. 
%
We see that we would compute the mcdf for $(4,[2,2])$ twice, but instead we can now read it from the MCDF cache.

Furthermore, if our example has symmetric minimum proportions $p_1 = p_1$, the mcdf of $(4,[2,1])$ is equal to $(4,[1,2])$ and $\textit{mcdf}(4,[1,3]) = \textit{mcdf}(4,[3,1])$. 
%
Generally, if $p_1 = p_2 = \cdots = p_{|G|}$, we can make use of the mTree's symmetry: we calculate the mcdf only for node $m(i)$ and store it in the cache \emph{as well as its mirror} (see Section~\ref{subsubsec:discarding-symmetric-nodes}), because their mcdf values are the same.

Note that the mcdf computation is only depends on $p_G$ and not on $alpha$. 
%
We can therefore persists the MCDF cache on disk for a particular vector $p_G$ and load it for any mTree calculation with the same $p_G$ in the future.
%
This also saves additional computation time during significance adjustment.

\subsubsection{Discarding symmetric nodes}
\label{subsubsec:discarding-symmetric-nodes}
In case of equal minimum proportions for all groups $p_1 = p_2 = \ldots = p_|G|$ the mTree shows a convenient property that we can use to reduce additional space and computation time. 
%
Remember that whenever the mcdf value falls below $\alpha$ for a particular position $i$, we have to put a protected candidate onto $i$. 
%
For equal minimum proportions the tree branches into $|G| - 1$ symmetric nodes $m(i)$ of the same likelyhood.
%
As an example reconsider the mTree from Figure~\ref{fig:mtree-symmetric-adjusted} at level 6. 
%
For two protected groups with minimum proportions $[1/3, 1/3]$ we see that the tree branches into two symmetric nodes $[1, 0]$ and $[0,1]$. 
%
Both have the same mcdf values.
%
We store only one of the nodes and flag it as ``has mirrored node'' and continue our mTree computation only in the stored branch.
%
This way we save half of the space and computation time needed, without loosing any information about the tree. 

\subsubsection{Stored mTrees}
\label{subsubsec:stored-mtrees}
During the computation of an adjusted mTree with parameters $k,p_G , \alpha$ we calculate many temporary mTrees (first the unadjusted ones, then the ones for the regression algorithm, then the ones for the binary search steps).
%
We persist all of the temporary mTrees plus the final tree in files for later usage.
%
The filenames contain the tree parameters and whether or not it is adjusted and its probability to fail a fair ranking $\failprob$.
%
If any of these trees is needed at a later point in time it can be loaded from disc instead of being recomputed, be it as input for multinomial \algoFAIR or as temporary tree during a new adjusted mTree computation.




