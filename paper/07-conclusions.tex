\section{Conclusions}\label{sec:conclusions}

In this paper we presented the extension of \algoFAIR to multiple groups, where we guarantee ranked group fairness, without introducing a large utility loss.
%
Especially when groups largely have the same utility score in the top positions (as is the case in the COMPAS experiments) no ranking utility at all is lost in terms of NDCG or individual fairness. 
%
In this case \algoFAIR only benefits the protected groups without skewing the ranking result.
%
If a protected group already receives advantageous exposure in the colorblind ranking and the ranked group fairness condition is already met via the ranking scores of the candidates, \algoFAIR preserves this. 
%
A protected candidate can only lose exposure due to a protected candidate from another group being ranked up, but not due to a non-protected one.
%
Additionally the user can control the degree of fairness that is obtained in the result by setting $p_G$ to a value that is appropriate for the situation at hand.
%
This lets them transparently control the trade-off between fairness and utility, instead of having a less intuitive fairness parameter $\theta$ that operates on the barycenter of group distributions.

\spara{Future work.}
%
An important challenge is the algorithmic complexity of calculating the mTree for a particular configuration of $p_G$ and $\alpha$.
%
Though we already implemented improvements to reduce complexity, calculating an adjusted mTree of length $k=100$ with six groups takes several weeks.
%
Of course this mTree has to be calculated only once and can then be persistent and shared among users of \algoFAIR, however when a situation demands a new configuration these calculation times are currently unavoidable.
%
A significant speed-up could be achieved by programming a customized mcdf-function which can store the results of repetitive computation steps. 
%
This is however very memory-intensive and the algorithm then needs to run on large computer clusters.
%
Additionally one could provide a script that fills the MCDF Cache with various configurations of $p_G$ and $\alpha$.
%
This calculation can continuously run as a separate process on a server which then provides the obtained caches to users who want to compute new mTrees.

One of the main challenges for fair ranking algorithms in general is that there is not yet any empirical evidence that re-ordering items actually helps to overcome the bias in click-probability across groups.
%
Very recent research \meike{cite Tom's project with Hima} suggests that there are several settings in which optimizing for exposure equity (as done by~\cite{biega2018equity, singh2019policy}) shows no effect on user biases in click through rates. 
%
Indeed early research indicates that when a protected group, such as women, receive equity of exposure in a ranking, the users searching for e.g. experts in certain domains still prefer men even when they are ranked to lower positions by a fairness algorithm.
%
A method that can increase the visibility beyond equity of exposure is needed to compensate for such persistent biases, such as \algoFAIR.
%
However further research has to be conducted to study the effect on users of re-ordering items in a ranking and to understand the best means to overcome these strong prejudices against minority groups in certain domains.

Experimentally, there are several directions. 
%
For instance, we have used real datasets that exhibit some differences among protected and non-protected groups; experiments with synthetic datasets would allow to test with more extreme differences that are more rarely found in real data.
%
Further experimental work may be done to measure robustness to noise in qualifications, and later to evaluate the impact of this in a real application.

\spara{Reproducibility.}
All the code and data used of this paper is available online at \url{https://github.com/MilkaLichtblau/Multinomial_FA-IR}.
