%!TEX root = main.tex
\section{Introduction}\label{sec:introduction}

People search engines are increasingly common for job recruiting and even for finding companionship or friendship.
%
A top-$k$ ranking algorithm is typically used to find the most suitable way of ordering items (persons, in this case), considering that if the number of people matching a query is large, most users will not scan the entire list.
%
Conventionally, these lists are ranked in descending order of some measure of the relative fitness of items, according to the \emph{probability ranking principle}~\cite{robertson1977probability}.
%

The main concern motivating this paper is that a machine learning ranking model may produce ranked lists that can systematically reduce the visibility of an already disadvantaged group~\cite{peder2008,Dwork2012}.
%
Disadvantaged groups correspond to legally protected categories such as people with disabilities, racial or ethnic minorities, or an under-represented gender in a specific professional domain.
%
Furthermore, it is assumed that this bias manifests differently across groups, rendering inter-group relevance scores incomparable with each other.
%

According to \citet{friedman1996bias} a computer system is \emph{biased} ``if it systematically and unfairly discriminate[s] against certain individuals or groups of individuals in favor of others.
%
A system discriminates unfairly if it denies an opportunity or a good or if it assigns an undesirable outcome to an individual or a group of individuals on grounds that are unreasonable or inappropriate.''
%
Yet ``unfair discrimination alone does not give rise to bias unless it occurs systematically'' and ``systematic discrimination does not establish bias unless it is joined with an unfair outcome.''
%
On a ranking, the desired good for an individual is to appear in the ranking and to be ranked amongst the top-$k$ positions.
%
The outcome can be deemed unfair if members of one or more protected groups are systematically ranked below those of a privileged group.
%
The ranking algorithm discriminates unfairly if this ranking decision is based fully or partially on the protected feature.
%
This discrimination is systematic when it is embodied in the algorithm's ranking model.
%
As shown in earlier research, a machine learning model trained on datasets incorporating \textit{preexisting bias} will embody this bias and therefore produce biased results, potentially increasing any disadvantage further, reinforcing existing bias~\cite{oneil2016weapons}.
%

Based on this observation, in this paper we study the problem of producing a ranking that we will consider fair given legally-protected attributes.
%
A formal definition of the problem studied can be found in Section~\ref{sec:problem}.
%
Intuitively, given a set $G$ of different minority groups, our aim is to produce a ranking in which the proportion of each group in $G$ in any ranking prefix does not fall below given minimum proportions $p_G$, where $p_G$ is a vector containing the respective minimum proportion~$p_g$ per group $g \in G$. In this ranking, we also would like to preserve relevance/utility as much as possible.
%
%We denote however the motivating assumption of our method which is that utility measures \emph{are not comparable across groups} because said bias manifests differently for each group.
%

We propose a re-ranking method to remove the systematic bias by means of a \emph{ranked group fairness criterion}, that we introduce in this paper.
%
We assume a ranking algorithm has given an undesirable outcome to one ore more groups of individuals, but the algorithm itself cannot determine if the grounds were appropriate or not.
%
Hence we expect the user of our method to know that the outcome is based on unreasonable or inappropriate grounds and provide the vector of proportions $p_G$ as input. This parameter can originate in a legal mandate or in voluntary affirmative actions.
%
For instance, the US Equal Employment Opportunity Commission sets a goal of 12\% of workers with disabilities in federal agencies in the US,\footnote{US EEOC: \url{https://www1.eeoc.gov/eeoc/newsroom/release/1-3-17.cfm}, Jan 2017.}
%
while in Spain, a minimum of 40\% of political candidates in voting districts exceeding a certain size must be women~\cite{verge2010gendering}.
%
In other cases, such quotas might be adopted voluntarily, for instance through a diversity charter.\footnote{European Commission: \url{http://ec.europa.eu/justice/discrimination/diversity/charters/}}
%
In general these measures do not mandate perfect parity, as distributions of qualifications across groups can be unbalanced for legitimate, explainable reasons~\cite{zliobaite2011handling,pedreschi2009integrating}. % pedreschi2009integrating has the truck driver license example }
%

The ranked group fairness criterion compares the number of protected elements in every prefix of the ranking with the expected number of protected elements if they were picked at random using a multinomially distributed statistical process (``dice rolls'' with each side $j$ of the dice representing a group, and having success probability $p_j \in p_G$).
%
Given that we use a statistical test for this comparison, we include a significance parameter $\alpha$ corresponding to the probability of a Type I error, which means rejecting a fair ranking.
%

\paragraph{Example.} Consider the three rankings in Table \ref{tbl:multinomial_intro_example} corresponding to a top-10 ranking of applicants in a credit approval process. The rankings are obtained based on the credit worthiness of each applicant taking into account different features such as account status, credit duration, and credit amount.
%
We also present to which protected group each individual belongs based on their demographics ``age'' and ``race.'' We observe that in the above rankings individuals belonging to the group of young or black applicants receive less visibility in the top-10 results than those of the non-protected group. This can lead to systematical disadvantages in accessing credit. 
More importantly, we see that individuals belonging to multiple protected groups (young \emph{and } black) receive the least exposure in the top-10 results.

	Considering the definition of \textit{ranked group fairness} proposed in \cite{zehlike2017fair} with minimum proportion $p=0.4$ and significance $\alpha=0.1$, these parameters translate into requiring at least one protected candidate in the top-5 positions of a ranking.
	Therefore the first ranking in Table \ref{tbl:multinomial_intro_example} will be considered as fair for the group of young individuals.
	%
	However the same ranking will be rejected as unfair for black individuals, as well as for the intersectional group definition of young and black.
	%
	Next, suppose that the required minimum proportion is $p = 0.3$ and $\alpha = 0.1$. This translates to having at least one protected candidate in the top-7 positions.
	When considering the second ranking in Table \ref{tbl:multinomial_intro_example} it will be accepted as fair for black individuals. Moreover, it will be also accepted as fair for young candidates but rejected as unfair for young black ones.
	%
	From the above we conclude that providing a fairness definition for rankings that treats protected groups separately can not guarantee a fair ranking for multiple protected groups at the same time. This highlights the need for a ranked group fairness notion for multiple protected groups ({\em i.e.}, multinomial).
	%
	The third ranking in Table \ref{tbl:multinomial_intro_example} is an example of a fair top-10 ranking for multiple protected groups young, non-white, and also young and non-white. It is created using our \textit{multinomial ranked group fairness} criteria and the algorithm proposed in this paper.
\medskip
\begin{table}[t]
	\caption{Three possible rankings of top-10 results involving people belonging to two age groups ``y'' and ``o'' and two racial groups ``w'' and ``b''. The first ranking would be considered fair for young people, but not for black people; the second ranking would be considered fair for black people, but not for young people; the third ranking would satisfy both criteria.
		%observe that the top-10 results under-represent the least represented gender, in comparison with top-40 results.
		%
		\label{tbl:multinomial_intro_example}}
	\centering\small\begin{tabular}{lcccc}\toprule
		Protected group & Position					  & top-10 & top-10  & top-10 \\
		%          & \multicolumn{1}{c}{Position}                      & 10   & 10     & 40 \\
		& \texttt{1 2 3 4 5 6 7 8 9 10} & young & black & y. and b. \\
		&                               &  (y)  & (b)  &  (y $\wedge$ b)\\
		\midrule
		Young  & \texttt{o/w o/w o/w y/w o/w o/w o/w o/w o/w o/w} & 10\% & 0\% & 0\% \\
		Black & \texttt{o/w o/w o/w y/w o/w o/b o/w o/w o/w o/w} & 10\% & 10\% & 0\% \\
		Young and Black & \texttt{o/w o/w o/w o/w o/w y/b o/w o/w o/w o/w} & 20\% & 10\% & 10\% \\
		\bottomrule
	\end{tabular}

\end{table}


%\medskip
%\begin{example} Consider the three rankings in Table \ref{tbl:xing_intro_example} corresponding to searches for an ``economist,'' ``market research analyst,'' and ``copywriter'' in XING\footnote{\url{https://www.xing.com/}}, an online platform for jobs that is used by recruiters and headhunters, mostly in German-speaking countries, to find suitable candidates in diverse fields (data collection is reported in detail in~\cite{zehlike2017fair}). While analyzing  the extent to which candidates of both sexes are represented as we go down these lists,  we can observe that such proportion keep changing and is not uniform (see, for instance, the top-10 vs. the top-40). As a consequence, recruiters examining these lists will see different proportions depending on the point at which they decide to stop.
%%
%This outcome systematically disadvantages individuals of one sex by preferring the other at the top-$k$ positions, a clear instance of bias in a computer system~\cite{friedman1996bias}. As we do not know the learning model behind the ranking, we assume that the result is at least partly based on the protected attribute \emph{sex}.
%
%\note{Why do we need this assumption? Imposing group-fairness constraint makes sense in any case. This assumption sounds like a not-needed justification to me.}
%
%Let $k = 10$. Our notion of \textit{ranked group fairness} imposes a fair representation with proportion $p$ and significance $\alpha$ at each top-$i$ position with $i \in [1,10]$ (formal definitions are given in section~\ref{sec:problem}).
%Consider for instance $\alpha = 0.1$ and suppose that the required proportion is $p = 0.4$.  This translates (see Table \ref{tbl:ranked_group_fairness_table}) to having at least one individual from the protected minority class in the first 5 positions: therefore the ranking for  ``copywriter'' would be rejected as unfair. However, it also requires to have at least 2 individuals from the protected group in the first 9 positions: therefore also the ranking for ``economist'' is rejected as unfair, while the ranking for ``market research analyst'' is fair for  $p = 0.4$. However, if we would require $p = 0.5$ then this translates in having at least 3 individuals from the minority group in the top-10, and thus even the ranking for ``market research analyst'' would be considered unfair.
%%
%We note that for simplicity, in this example we have not adjusted the significance $\alpha$ to account for multiple statistical tests; this is not trivial, and is one of the key contributions of this paper.
%\end{example}
%\medskip
%
%\begin{table}[t]
%	\caption{Example of non-uniformity of the top-10 vs. the top-40 results for different queries in XING (Jan 2017).
%		%``economist,'' ``market research analyst,'' and ``copywriter'' in XING (Jan 2017). We %do not claim anything about the merits of these proportions, but simply
%		%observe that the top-10 results under-represent the least represented gender, in comparison with top-40 results.
%		\label{tbl:xing_intro_example}}
%
%	\centering\small\begin{tabular}{lccccc}\toprule
%		& Position					  & top 10 & top 10  & top 40 & top 40 \\
%		%          & \multicolumn{1}{c}{Position}                      & 10   & 10     & 40   & 40 \\
%		& \texttt{1 2 3 4 5 6 7 8 9 10} & male & female & male & female \\
%		\midrule
%		Econ.  & \texttt{f m m m m m m m m m} & 90\% & 10\% & 73\% & 27\% \\
%		Analyst& \texttt{f m f f f f f m f f} & 20\% & 80\% & 43\% & 57\% \\
%		Copywr.& \texttt{m m m m m m f m m m} & 90\% & 10\% & 73\% & 27\% \\
%		\bottomrule
%	\end{tabular}
%
%
%\end{table}
%
%\todo{We would need another example with multiple protected attributes.}
%


We remark that our method is suitable for concerns about \emph{intersectionality}, which refers to the fact that the personal, political, and social identities of an individual can be combined to form a unique profile that can be discriminated.
%
Consider as an example the case DeGraffenreid v. General Motors in the US in 1976: here Emma DeGraffenreid wanted to challenge General Motors for being discriminated in the promotion procedure for being a black woman.
%
However the court found that GM was neither discriminated against blacks, nor against women and therefore ruled that DeGraffenreid ``could not combine the claims'' of race and sex discrimination.
%
Yet many cases of intersectional discrimination demand for a method like ours that is capable of dealing with biases against groups that are oppressed based on more than one protected attribute.

\subsection{Contributions and Roadmap}

This work is the extended version of our previous conference paper~\cite{zehlike2017fair}.
%
In this extended version we define and analyze the {\sc Fair Top-$k$ Ranking problem} with multiple groups, in which we want to determine a subset of $k$ candidates from a large pool of $n \gg k$ candidates, in a way that maintains high utility (selects the ``best'' candidates from each group), subject to a group fairness criteria.
%
The algorithm that addresses multiple protected groups is completely different from the simpler algorithm that we used for the case of a single protected group.
%
The running example we use in this paper is that of selecting automatically, from a large pool of potential candidates, a smaller group that will be interviewed for a position.
%

Our notion of utility assumes that we want to invite the most qualified candidates from each group, while their qualification is equal to a relevance score calculated by a ranking algorithm.
%
This score is assumed to be based on relevant metrics for evaluating candidates for a position, which depending on the specific skills required for the job could be their grades ({\em e.g.}, Grade Point Average), their results in a standardized knowledge/skills test specific for a job, such as their typing speed in words per minute for typists, or their number of hours of flight in the case of pilots.
%
We note that this measurement will embody \emph{pre-existing bias} ({\em e.g.}, if black pilots are given less opportunities to flight they accumulate less flight hours), as well as \emph{technical bias}, as learning algorithms are known to be susceptible to direct and indirect discrimination~\cite{tuto2016,HajianFerrer12}.
%
We furthermore note that different manifestations of such bias exists for each group and are usually stronger for intersectional groups, that is, the pre-existing bias against black women is stronger than the one for women or blacks in general.
%

This utility principle is operationalized in two ways.
%
First, by a criterion we call \emph{selection utility}, which prefers rankings in which every candidate included in the top-$k$ is more qualified than every candidate not included, or in which the difference in their qualifications is small.
%
Second, by a criterion we call \emph{ordering utility}, which prefers rankings in which for every pair of candidates included in the top-$k$, either the more qualified candidate is ranked above, or the difference in their qualifications is small.
%
Note however, that in a setting with multiple protected groups, optimal selection and ordering utility cannot be guaranteed because of said differences in the group skews.
%
Mathematically this means that the optimal solution for multinomial ranked group fairness ({\em i.e.}, for more than one protected group) is a solution space rather than just a single point as it was in~\cite{zehlike2017fair}, while the optimal solution in terms of utility is still a single point within said solution space whose location depends on the candidate set at hand.
%
We want to stress that trying to find this point of optimal utility corresponds to a worldview in which one assumes that utility measures of candidates across different groups are actually comparable and that the per-group bias is known a-priori.
%
We believe that the group skew unawareness is a necessary condition for the justification of post-processing algorithms in general and we therefore explicitly do not search for the optimal solution in terms of utility.
%
We will go into more depth on this in Section~\ref{subsec:individual-fairness}.
%

Our definition of \emph{ranked group fairness} reflects the legal principle of group under-representation in obtaining a benefit \cite{ellis2012eu,lerner2003group}.
%
We use the standard notion of protected groups ({\em e.g.}, ``people with disabilities''); where protection emanates from a legal mandate or a voluntary commitment.
%
%The group under-representation principle, and the related disparate impact doctrine~\cite{Barocas2014} addresses the fact that there might be differences in qualification across different groups by \emph{not} mandating an equal proportion of candidates from the protected group and non-protected group in the output. It simply states that the proportions cannot be too different.
%
We formulate a criterion by applying a statistical test on the proportion of protected candidates on every prefix of the ranking, which should be indistinguishable or above a given minimum.
%
%This procedure can be seen as a form of positive action to ensure that the proportion of protected candidates in every prefix of the top-$k$ ranking is statistically indistinguishable from a policy target.
%
We also show that the verification of the ranked group fairness criterion can be implemented efficiently by pre-computing a verification data structure that we call \emph{mTree}.
%
This tree contains all possibilities to create a ranking that satisfies ranked group fairness and we provide an algorithm to build and persist it.
%
We also provide an algorithm \algoCorrect for the mTree adjustment due to multiple dependent hypothesis testing, where we compute $\alphaadj$ such that the type-I-error is less or equal to $\alpha$ for the \emph{entire tree}.
%

Finally, we propose an efficient algorithm, named \algoFAIR, for producing a top-$k$ ranking that maintains high utility while satisfying ranked group fairness, as long as there are ``enough'' protected candidates from each group to achieve the desired minimum proportions.
%
Note that if a group of protected candidates is too small to satisfy ranked group fairness, the ranking is necessarily bound to under-represent them.
%
We also present extensive experiments to evaluate the performance of our approach compared to a group-unaware method (the so-called ``color-blind'' method) with respect to both, the expected utility of a ranking and the fairness degree measured, in terms of expected exposure.

Summarizing, the main contributions of this paper are:
\begin{compactenum}
	\item the principled definition of \emph{ranked group fairness} for multiple protected groups, and the associated  {\sc Fair Top-$k$ Ranking problem};
	\item the \algoFAIR\ algorithm for multiple protected groups to produce a top-$k$ ranking that maximizes utility while satisfying ranked group fairness.
	\item a mathematical framework to solve the problem of multiple \emph{dependent} hypotheses testing, which is to adjust the significance level $\alphaadj$ such that the  maximum probability for a type-1-error is the given significance level $\alpha$. 
	\item the algorithms \algoComputeMTree and \algoCorrect which translate ranked group fairness into a pre-computed data structure. This will enable efficient verification of ranked group fairness at testing time and efficient creation of fair rankings.
\end{compactenum}

Our method can be used within an anti-discrimination framework such as \emph{affirmative actions}~\cite{sowell2005affirmative}.
%
We do not claim these are the only way of achieving fairness, but we provide \emph{an algorithm grounded in statistical tests that enables the implementation of a positive action policy in the context of ranking}.

The rest of this paper is organized as follows.
%
The next section presents a brief survey of related literature,
%
while Section~\ref{sec:problem} introduces our ranked group fairness and utility criteria and a formal problem statement.
%
Section~\ref{sec:fairness-verification} presents a data structure (mTree) that allows fast verification of the ranked group fairness criterion.
%
Section~\ref{sec:model-adjustment} presents a procedure for statistical test significance adjustment, which is required due to multiple hypotheses testing.
%
Section~\ref{sec:multinom-fair-algo} describes the \algoFAIR\ algorithm for fair rankings.
%
Section~\ref{sec:experiments} presents experimental results.
%
Section~\ref{sec:adjustment-binomial} describes a significance adjustment procedure for a single protected group (a improvement of the method shown in \cite{zehlike2017fair}), which is faster and simpler than for multiple protected groups.
%
Section~\ref{sec:conclusions} presents our conclusions and future work.
