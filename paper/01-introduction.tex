%!TEX root = main.tex
\section{Introduction}\label{sec:introduction}

People search engines are increasingly common for job recruiting and even for finding companionship or friendship.
%
A top-$k$ ranking algorithm is typically used to find the most suitable way of ordering items (persons, in this case), considering that if the number of people matching a query is large, most users will not scan the entire list.
%
Conventionally, these lists are ranked in descending order of some measure of the relative fitness of items, according to the \emph{probability ranking principle}~\cite{robertson1977probability}.

The main concern motivating this paper is that a machine learning ranking model may produce ranked lists that can systematically reduce the visibility of an already disadvantaged group~\cite{peder2008,Dwork2012}.
%
Disadvantaged groups correspond to legally protected categories such as people with disabilities, racial or ethnic minorities, or an under-represented gender in a specific professional domain.
%
Furthermore it is assumed that this bias manifests differently across groups, rendering inter-group relevance scores incomparable with each other.


According to \citet{friedman1996bias} a computer system is \emph{biased} ``if it systematically and unfairly discriminate[s] against certain individuals or groups of individuals in favor of others.
%
A system discriminates unfairly if it denies an opportunity or a good or if it assigns an undesirable outcome to an individual or a group of individuals on grounds that are unreasonable or inappropriate.''
%
Yet ``unfair discrimination alone does not give rise to bias unless it occurs systematically'' and ``systematic discrimination does not establish bias unless it is joined with an unfair outcome.''
%
On a ranking, the desired good for an individual is to appear in the result and to be ranked amongst the top-$k$ positions.
%
The outcome can be deemed unfair if members of one or more protected groups are systematically ranked lower than those of a privileged group.
%
The ranking algorithm discriminates unfairly if this ranking decision is based fully or partially on the protected feature.
%
This discrimination is systematic when it is embodied in the algorithm's ranking model.
%
As shown in earlier research, a machine learning model trained on datasets incorporating \textit{preexisting bias} will embody this bias and therefore produce biased results, potentially increasing any disadvantage further, reinforcing existing bias~\cite{oneil2016weapons}.


Based on this observation, in this paper we study the problem of producing a ranking that we will consider fair given legally-protected attributes.
%
A formal definition \textcolor[rgb]{0.00,0.00,1.00}{of the problem studied} can be found in Section~\ref{sec:problem}: \textcolor[rgb]{0.00,0.00,1.00}{intuitively, our aim is} to produce a ranking in which the proportion of different minority groups in any prefix of the ranking does not fall below minimum proportions $p_G$. In this ranking, we also would like to preserve relevance/utility as much as possible.
%
%We denote however the motivating assumption of our method which is that utility measures \emph{are not comparable across groups} because said bias manifests differently for each group.

We propose a \textcolor[rgb]{0.00,0.07,1.00}{re-ranking} method to remove the systematic bias by means of a \emph{ranked group fairness criterion}, that we introduce in this paper.
%
We assume a ranking algorithm has given an undesirable outcome to one ore more groups of individuals, but the algorithm itself cannot determine if the grounds were appropriate or not.
%
Hence we expect the user of our method to know that the outcome is based on unreasonable or inappropriate grounds and provide $p_G$ as input which can originate in a legal mandate or in voluntary commitments.
%
For instance, the US Equal Employment Opportunity Commission sets a goal of 12\% of workers with disabilities in federal agencies in the US,\footnote{US EEOC: \url{https://www1.eeoc.gov/eeoc/newsroom/release/1-3-17.cfm}, Jan 2017.}
%
while in Spain, a minimum of 40\% of political candidates in voting districts exceeding a certain size must be women~\cite{verge2010gendering}.
%
In other cases, such quotas might be adopted voluntarily, for instance through a diversity charter.\footnote{European Commission: \url{http://ec.europa.eu/justice/discrimination/diversity/charters/}}
%
In general these measures do not mandate perfect parity, as distributions of qualifications across groups can be unbalanced for legitimate, explainable reasons~\cite{zliobaite2011handling,pedreschi2009integrating}. % pedreschi2009integrating has the truck driver license example }


The ranked group fairness criterion compares the number of protected elements in every prefix of the ranking with the expected number of protected elements if they were picked at random using a multinomially distributed statistical process (``dice rolls'' with each side $ j $ of the dice having success probability $p_j \in p_G$).
%
Given that we use a statistical test for this comparison, we include a significance parameter $\alpha$ corresponding to the probability of a Type I error, which means rejecting a fair ranking in this test.

\begin{example} Consider the three rankings in Table \ref{tbl:multinomial_intro_example} corresponding to top 10 ranking applicants in a credit approval process. The rankings are obtained based on the credit worthiness of each applicant taking into different account features such as account status, credit duration andcredit amount. 
	We also present to which protected group each individual belonged w.r.t. their demographics features such as age and race. As observed, the above rankings show that 
	individuals belonged to protected groups have less exposure in the top 10 results than favored group which can lead to systematically disadvantages individuals of one protected group (e.g., younger than 30 or not-white) to others (between 30 and 50, older than 50 or white) in accessing credit. Additionally, individuals belonged to multiple protected groups (younger than 30 and not white) have the least exposure in the top 10 results.
	
	More specifically, we show if the first ranking in Table \ref{tbl:multinomial_intro_example} can pass the test of \textit{ranked group fairness} proposed in ~\cite{zehlike2017fair} for one protected group (younger than 25 here). Suppose that the required proportion for this group is $p = 0.4$ and significant value is $\alpha = 0.1$, this translates to having at least one individual from the protected group in the first 5 positions: therefore the first ranking in table 1 would be accepted as fair for young age group but would be rejected as unfair for not-white group and also young and not-white group for the same value of $p$ and $a$.
	
	Considering the second ranking in Table \ref{tbl:multinomial_intro_example} one not-white protected group and suppose that the required proportion for this group is $p = X$ and significant value is $\alpha = Y$. This translates to having at least one individual from the protected group in the first 6 positions: therefore the second ranking in table 1 would be accepted as fair for not-white group. However, assume that 
	the required proportion for young group is $p = 0.4$, the second ranking would be also accepted as fair for younger than 25 group, however, it would be rejected as unfair for not-white and young group for either vaalue of p.
	
	From the above we can conclude that providing the fair ranking results for each protected group can not guarantee the fair ranking for multiple protected groups. This highlights the need to ranked group fairness notion taking into account multiple protected groups (i.e., multinomial). As an example, the third ranking in Table \ref{tbl:multinomial_intro_example} is an example of the top 10 ranking that achieves the protection for multiple protected groups: young, not-white and also young and not-white using our \textit{multinomial ranked group fairness} criteria and algorithm proposed in this paper. 
	
\end{example}
\medskip

\begin{table}[t]
	\caption{Example of top-10 results for different protected groups in a credit approval process.
		%observe that the top-10 results under-represent the least represented gender, in comparison with top-40 results.
		%
		\label{tbl:multinomial_intro_example}}
	
	\centering\small\begin{tabular}{lcccc}\toprule
		& Position					  & top 10 & top 10  & top 10 \\
		%          & \multicolumn{1}{c}{Position}                      & 10   & 10     & 40 \\
		& \texttt{1 2 3 4 5 6 7 8 9 10} & young & not-white & y\& \\
		&                               &  (y)  & (nw)  &  nw \\
		\midrule
		Young  & \texttt{o/w o/w o/w y/w o/w o/w o/w o/w o/w o/w} & 10\% & 0\% & 0\% \\
		not-white & \texttt{o/w o/w o/w y/w o/w o/nw o/w o/w o/w o/w} & 10\% & 10\% & 0\% \\
		young and not-white & \texttt{o/w o/w o/w o/w o/w y/nw o/w o/w o/w o/w} & 20\% & 10\% & 10\% \\
		\bottomrule
	\end{tabular}
	
\end{table}


\medskip
\begin{example} Consider the three rankings in Table \ref{tbl:xing_intro_example} corresponding to searches for an ``economist,'' ``market research analyst,'' and ``copywriter'' in XING\footnote{\url{https://www.xing.com/}}, an online platform for jobs that is used by recruiters and headhunters, mostly in German-speaking countries, to find suitable candidates in diverse fields (data collection is reported in detail in~\cite{zehlike2017fair}). While analyzing  the extent to which candidates of both sexes are represented as we go down these lists,  we can observe that such proportion keep changing and is not uniform (see, for instance, the top-10 vs. the top-40). As a consequence, recruiters examining these lists will see different proportions depending on the point at which they decide to stop.
%
This outcome systematically disadvantages individuals of one sex by preferring the other at the top-$k$ positions, a clear instance of bias in a computer system~\cite{friedman1996bias}. As we do not know the learning model behind the ranking, we assume that the result is at least partly based on the protected attribute \emph{sex}.

\note{Why do we need this assumption? Imposing group-fairness constraint makes sense in any case. This assumption sounds like a not-needed justification to me.}

Let $k = 10$. Our notion of \textit{ranked group fairness} imposes a fair representation with proportion $p$ and significance $\alpha$ at each top-$i$ position with $i \in [1,10]$ (formal definitions are given in section~\ref{sec:problem}).
Consider for instance $\alpha = 0.1$ and suppose that the required proportion is $p = 0.4$.  This translates (see Table \ref{tbl:ranked_group_fairness_table}) to having at least one individual from the protected minority class in the first 5 positions: therefore the ranking for  ``copywriter'' would be rejected as unfair. However, it also requires to have at least 2 individuals from the protected group in the first 9 positions: therefore also the ranking for ``economist'' is rejected as unfair, while the ranking for ``market research analyst'' is fair for  $p = 0.4$. However, if we would require $p = 0.5$ then this translates in having at least 3 individuals from the minority group in the top-10, and thus even the ranking for ``market research analyst'' would be considered unfair.
%
We note that for simplicity, in this example we have not adjusted the significance $\alpha$ to account for multiple statistical tests; this is not trivial, and is one of the key contributions of this paper.
\end{example}
\medskip

\begin{table}[t]
	\caption{Example of non-uniformity of the top-10 vs. the top-40 results for different queries in XING (Jan 2017).
		%``economist,'' ``market research analyst,'' and ``copywriter'' in XING (Jan 2017). We %do not claim anything about the merits of these proportions, but simply
		%observe that the top-10 results under-represent the least represented gender, in comparison with top-40 results.
		\label{tbl:xing_intro_example}}

	\centering\small\begin{tabular}{lccccc}\toprule
		& Position					  & top 10 & top 10  & top 40 & top 40 \\
		%          & \multicolumn{1}{c}{Position}                      & 10   & 10     & 40   & 40 \\
		& \texttt{1 2 3 4 5 6 7 8 9 10} & male & female & male & female \\
		\midrule
		Econ.  & \texttt{f m m m m m m m m m} & 90\% & 10\% & 73\% & 27\% \\
		Analyst& \texttt{f m f f f f f m f f} & 20\% & 80\% & 43\% & 57\% \\
		Copywr.& \texttt{m m m m m m f m m m} & 90\% & 10\% & 73\% & 27\% \\
		\bottomrule
	\end{tabular}


\end{table}

\todo{We would need another example with multiple protected attributes.}



We remark that our method is suitable for concerns about \emph{intersectionality}, which refers to the fact that \textcolor[rgb]{0.00,0.00,1.00}{the personal}, political, and social identities \textcolor[rgb]{0.00,0.00,1.00}{of an individual can be combined to form a unique profile that can be discriminated}.
%
Consider as an example the case DeGraffenreid v. General Motors in 1976: here Emma DeGraffenreid wanted to challenge General Motors for being discriminated in the promotion procedure for being a black woman.
%
However the court found that GM had neither discriminated against blacks, nor against women and therefore ruled that DeGraffenreid ``could not combine the claims'' of race and sex discrimination.
%
Yet many cases of intersectional discrimination demand for a method like ours that is capable of dealing with biases against groups that are oppressed based on more than one protected \textcolor[rgb]{0.00,0.00,1.00}{attribute}.

\textcolor[rgb]{0.00,0.00,1.00}{\subsection{Contributions and Roadmap}}
\textcolor[rgb]{0.00,0.00,1.00}{This work presents an extension of our previous paper~\cite{zehlike2017fair}, which was published at the CIKM 2017 conference. In this extended version} we define and analyze the {\sc Fair Top-$k$ Ranking problem} with multinomial groups, in which we want to determine a subset of $k$ candidates from a large pool of $n \gg k$ candidates, in a way that maintains high utility (selects the ``best'' candidates from each group), subject to group fairness criteria.
%
The running example we use in this paper is that of selecting automatically, from a large pool of potential candidates, a smaller group that will be interviewed for a position.

Our notion of utility assumes that we want to invite the most qualified candidates from each group, while their qualification is equal to a relevance score calculated by a ranking algorithm.
%
This score is assumed to be based on relevant metrics for evaluating candidates for a position, which depending on the specific skills required for the job could be their grades (e.g., Grade Point Average), their results in a standardized knowledge/skills test specific for a job, their typing speed in words per minute for typists, or their number of hours of flight in the case of pilots.
%
We note that this measurement will embody \emph{pre-existing bias} (e.g. if black pilots are given less opportunities to flight they accumulate less flight hours), as well as \emph{technical bias}, as learning algorithms are known to be susceptible to direct and indirect discrimination~\cite{tuto2016,HajianFerrer12}.
%
We furthermore note that different manifestations of such bias exists for each group and are usually stronger for intersectional groups, i.e. the pre-existing bias against black women is stronger than the one for women or blacks in general.

This utility principle is operationalized in two ways.
%
First, by a criterion we call \emph{selection utility}, which prefers rankings in which every candidate included in the top-$k$ is more qualified than every candidate not included, or in which the difference in their qualifications is small.
%
Second, by a criterion we call \emph{ordering utility}, which prefers rankings in which for every pair of candidates included in the top-$k$, either the more qualified candidate is ranked above, or the difference in their qualifications is small.
%
Note however, that in a setting with multiple protected groups optimal selection and ordering utility cannot be guaranteed because of said differences in the group skews.
%
Mathematically this means that the optimal solution for multinomial ranked group fairness (i.e. for more than one protected group) is a solution space rather than just a single point as it was in~\cite{zehlike2017fair}, while the optimal solution in terms of utility is still a single point within said solution space whose location depends on the candidate set at hand.
%
We want to stress that trying to find this point of optimal utility corresponds to a worldview in which one assumes that utility measures of candidates across different groups are actually comparable and that the per-group bias is known a-priori.
%
We believe that the group skew unawareness is a necessary condition for the justification of post-processing algorithms in general and we therefore explicitly do not search for the optimal solution in terms of utility.
%
We will go into more depth on this in Section~\ref{subsec:individual-fairness}.

Our definition of \emph{ranked group fairness} reflects the legal principle of group under-representation in obtaining a benefit \cite{ellis2012eu,lerner2003group}.
%
We use the standard notion of protected groups (e.g., ``people with disabilities''); where protection emanates from a legal mandate or a voluntary commitment.
%
%The group under-representation principle, and the related disparate impact doctrine~\cite{Barocas2014} addresses the fact that there might be differences in qualification across different groups by \emph{not} mandating an equal proportion of candidates from the protected group and non-protected group in the output. It simply states that the proportions cannot be too different.
%
We formulate a criterion by applying a statistical test on the proportion of protected candidates on every prefix of the ranking, which should be indistinguishable or above a given minimum.
%
%This procedure can be seen as a form of positive action to ensure that the proportion of protected candidates in every prefix of the top-$k$ ranking is statistically indistinguishable from a policy target.
%
We also show that the verification of the ranked group fairness criterion can be implemented efficiently by pre-computing a verification data structure that we call \emph{mTree}.
%
This tree contains all possibilities to create a ranking that satisfies ranked group fairness and we provide an algorithm to build and persist it.
%
We also provide an algorithm \algoCorrect for the mTree adjustment due to multiple dependent hypothesis testing, where we compute $\alphaadj$ such that the type-I-error is less or equal to $\alpha$ for the \emph{entire tree}.

Finally, we propose an efficient algorithm, named \algoFAIR, for producing a top-$k$ ranking that maintains high utility while satisfying ranked group fairness, as long as there are ``enough'' protected candidates from each group to achieve the desired minimum proportions.
%
We also present extensive experiments to evaluate the performance of our approach compared to the so-called ``color-blind'' ranking with respect to both the expected utility of a ranking and the fairness degree measured in terms of expected exposure.

\medskip

Summarizing, the main contributions of this paper are:
\begin{compactenum}
	\item the principled definition of \emph{ranked group fairness} for multiple protected groups, and the associated  {\sc Fair Top-$k$ Ranking problem};
	\item the \algoComputeMTree algorithm to provide the mTree, a data structure that \algoFAIR needs as input
	\item the \algoCorrect algorithm to find the adjusted mTree, which has a maximum probability for a type-1-error of $\alpha$
	\item the \algoFAIR\ algorithm for producing a top-$k$ ranking that maximizes utility while satisfying ranked group fairness.
\end{compactenum}

Our method can be used within an anti-discrimination framework such as \emph{positive actions}~\cite{sowell2005affirmative}.
%
We do not claim these are the only way of achieving fairness, but we provide \emph{an algorithm grounded in statistical tests that enables the implementation of a positive action policy in the context of ranking}.


\todo{Summarise in details the differences between the conference version and this extended one.}



The rest of this paper is organized as follows.

\todo{The roadmap below should be revised and extended a bit once we're totally sure about the structure of the paper.}

The next section presents a brief survey of related literature, while Section~\ref{sec:problem} introduces our ranked group fairness and utility criteria, our model adjustment approach, and a formal problem statement.
%
Section~\ref{sec:multinom-fair-algo} describes the \algoFAIR\ algorithm and the model adjustment algorithm.
%
Section~\ref{sec:experiments} presents experimental results.
%
Section~\ref{sec:conclusions} presents our conclusions and future work.
