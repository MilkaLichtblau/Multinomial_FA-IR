\section{Experiments}\label{sec:experiments}
\todo{Are we still doing this experiment to verify algorithm correctness?}
In the first part of our experiments we create synthetic datasets to demonstrate the correctness of the adjustment done by Algorithm \algoCorrect (\S\ref{subsubsec:JuliaExperimentalVerification}).
%
In the second part, we consider several public datasets for evaluating algorithm \algoFAIR (datasets in \S\ref{sec:experiments-datasets}, metrics and comparison with baselines in \S\ref{sec:experiments-baselines}, and results in \S\ref{sec:experiments-results}).

\subsection{Verification of Multiple Tests Adjustment}
\label{subsubsec:JuliaExperimentalVerification}

We empirically verified the adjustment formula and the \algoCorrect method using randomly generated data.
%
We repeatedly generated multiple rankings of different lengths $k$ using the algorithm by \citet{yang2016measuring} and evaluated these rankings with our ranked group fairness test, determining the probability that this ranking, which we consider fair, was declared unfair.
%
Example results are shown on Figure~\ref{fig:julia-experimental-verification} for some combinations of $k$ and $\alphaadj$.
%
As expected, the experiment results closely resemble the output of \algoCorrect.
%test's failure probability. Figure \inote[Meike]{insert figure} shows the result at a constant significance level $\alpha=0.1$ and a fairness probability of $p=0.5$. Figure \inote[Meike]{insert figure} shows the same experiment with a corrected significance level $\alphaadj$. We see that using a corrected $\alphaadj$ at each prefix instead of $\alpha$ gives us a constant failure probability even when $k$ increases.

\iffalse
% DATA TABLE FOR FIGURE (model, i.e., line)
k,alphaadj,p,alphamodel
1000,0.01,0.10,0.075378
1000,0.01,0.20,0.090049
1000,0.01,0.30,0.098331
1000,0.01,0.40,0.100432
1000,0.01,0.50,0.103713
1000,0.01,0.60,0.103976
1000,0.01,0.70,0.105475
1000,0.01,0.80,0.103502
1000,0.01,0.90,0.099602
1500,0.05,0.10,0.295883
1500,0.05,0.20,0.330252
1500,0.05,0.30,0.349234
1500,0.05,0.40,0.361767
1500,0.05,0.50,0.360710
1500,0.05,0.60,0.362456
1500,0.05,0.70,0.360749
1500,0.05,0.80,0.356852
1500,0.05,0.90,0.328008
\fi


\begin{figure}[t]
	\includegraphics[width=.55\columnwidth]{pics/FailureProbability10000Trials.png}
	\vspace{-2mm}
	\caption{Probability of considering a fair ranking generated by~\cite{yang2016measuring} as unfair for $k=1,000; \alphaadj=0.01$ (bottom curve) and for $k=1,500; \alphaadj=0.05$ (top curve). Model represented by lines, experimental results (avg. of 10,000 runs) by crosses.}
	\vspace{-5mm}
	\label{fig:julia-experimental-verification}
\end{figure}

\begin{table}[t]
	\caption{Datasets and experimental settings.}
	\vspace{-3mm}
	\label{tbl:datasets}
	\resizebox{1.01\columnwidth}{!}{%
		\centering\begin{tabular}{clcccccc}\toprule
			&        &                         &                         & Quality   & Protected & Protected \\
			& Dataset & \multicolumn{1}{c}{$n$} & \multicolumn{1}{c}{$k$} & criterion & group     & \% \\ \midrule
			D1 & COMPAS \cite{angwin_2016_machine}& 18K  & 1K & $\neg$recidivism & Afr.-Am. & 51.2\% \\
			D2 & "  & "  & " & " & male & 80.7\%\\ 
			D3 & "  & "  & " & " & female & 19.3\%\\ 
			D4 & Ger. credit \cite{lichman_2013_uci} & 1K   & 100   & credit rating & female & 69.0\% \\
			D5 & " & " & " & " & $<$ 25 yr. & 14.9\% \\
			D6 & "  & " & " & " & $<$ 35 yr. & 54.8\% \\ 
			D7 & SAT \cite{sat_2014}   & 1.6 M & 1.5K  & test score  & female & 53.1\%  \\ 
			D8 & XING [ours]           & 40 & 40 & ad-hoc score  & f/m/f & 27/43/27\%  \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-3mm}
\end{table}

\subsection{Datasets}\label{sec:experiments-datasets}

Table~\ref{tbl:datasets} summarizes the datasets used in our experiments.
%
Each dataset contains a set of people with demographic attributes, plus a quality attribute.
%
For each dataset, we consider a value of $k$ that is a small round number  ({\em e.g.}, 100, 1,000, or 1,500), or $k=n$ for a small dataset.
%
For the purposes of these experiments, we considered several scenarios of protected groups.
%
We remark that the choice of protected group is not arbitrary: it is determined completely by law or voluntary commitments; for the purpose of experimentation we test different scenarios, but in a real application there is no ambiguity about which is the protected group and what is the minimum proportion.
%
An experiment consists of generating a ranking using \algoFAIR and then comparing it with baseline rankings according to the metrics introduced in the next section.

We used the two publicly-available datasets used in \cite{yang2016measuring} (COMPAS~\cite{angwin_2016_machine} and German Credit~\cite{lichman_2013_uci}), plus another publicly available dataset (SAT~\cite{sat_2014}), plus a new dataset created and released with this paper (XING), as we describe next.

\spara{COMPAS} (Correctional Offender Management Profiling for Alternative Sanctions) is an assessment tool for predicting recidivism based on a questionnaire of 137 questions. It is used in several jurisdictions in the US, and has been accused of racial discrimination by producing a higher likelihood to recidivate for African Americans~\cite{angwin_2016_machine}.
%
In our experiment, we test a scenario in which we want to create a fair ranking of the top-$k$ people who are least likely to recidivate, who could be, for instance, considered for a pardon or reduced sentence.
%
We observe that African Americans as well as males are given a larger recidivism score than other groups; for the purposes of this experiment we select these two categories as the protected groups.

\spara{German Credit} is the Statlog German Credit Data collected by Hans Hofmann~\cite{lichman_2013_uci}.
%
It is based on credit ratings generated by Schufa, a German private credit agency based on a set of variables for each applicant, including age, gender, marital status, among others. Schufa Score is an essential determinant for every resident in Germany when it comes to evaluating credit rating before getting a phone contract, a long-term apartment rental or almost any loan.
%
We use the credit-worthiness as qualification, as~\cite{yang2016measuring}, and note that women and younger applicants are given lower scores; for the purposes of these experiments, we use those groups as protected.

\spara{SAT} corresponds to scores in the US Scholastic Assessment Test, a standardized test used for college admissions in the US.
We generate this data using the actual distribution of SAT results from 2014, which is publicly available for 1.6 million applicants in fine-grained buckets of 10 points (out of a total of 2,400 points)~\cite{sat_2014}.
%
The qualification attribute is set to be the achieved SAT score, and the protected group is women (female students), who scored about 25 points lower on average than men in this test.

\spara{XING} (\url{https://www.xing.com/}) is a career-oriented website from which we automatically collected the top-40 profiles returned for 54 queries, using three for which there is a clear difference between top-10 and top-40.
%
We used a non-personalized (not logged in) search interface and confirmed that it yields the same results from different locations.\label{concept:XING}
%
For each profile, we collected gender, list of positions held, list of education details, and the number of times each profile has been viewed in the platform, which is a measure of popularity of the profile.
%
With this information, we constructed an ad-hoc score: 
the months of work experience %in all the positions held by a person, 
plus the months of education, %
multiplied by the number of views of the profile.
%
This score tends to be somewhat higher for profiles in the first positions of the search results, but in general does not approximate the proprietary ordering in which profiles are shown. %, which is the result of a proprietary algorithm.
%
We include this score and its components in our anonymized data release.
%
We use the appropriate gender for each query as the protected group.

\subsection{Baselines and Metrics}\label{sec:experiments-baselines}

For each dataset, we generate various top-$k$ rankings with varying targets of minimum proportion of protected candidates $p$ using \algoFAIR, plus two baseline rankings:

\spara{Baseline 1: Color-blind ranking.} The ranking $c|_k$ that only considers the qualifications of the candidates, without considering group fairness, as described in Section~\ref{concept:color-blind-ranking}.

\spara{Baseline 2: \citet{Feldman2015}.} This ranking method aligns the probability distribution of the protected candidates with the non-protected ones. Specifically, for a candidate $i$ in the protected group, we replace its score $q_i \leftarrow q_j$ by choosing a candidate $j$ in the non-protected group having $F_n(j) = F_p(i)$, with $F_p(\cdot)$ (respectively, $F_n(\cdot))$ being the quantile of a candidate among the protected (respectively, non-protected) candidates.

\spara{Utility.} We report the loss in ranked utility after score normalization, in which all $q_i$ are normalized to be within $[0, 1]$.
%
We also report the maximum rank drop, {\em i.e.}, the number of positions lost by the candidate that realizes the maximum ordering utility loss.

\spara{NDCG.}
%
We report a normalized weighted summation of the quality of the elements in the ranking, $\sum_{i=1}^{k} w_i q_{(\tau_i)}$, in which the weights are chosen to have a logarithmic discount in the position:  $w_i = \frac{1}{\log_2 (i+1)}$. This is a standard measure to evaluate search rankings~\cite{jarvelin2002cumulated}.
%
This is normalized so that the maximum value is $1.0$.

\subsection{Results}\label{sec:experiments-results}

\begin{table}[t]
	\caption{Experimental results, highlighting in boldface the best non-color-blind result. Both FA*IR and the baseline from \citeauthor{Feldman2015} achieve the same target proportion of protected elements in the output and the same selection unfairness, but in general FA*IR achieves it with less ordering unfairness, and with less maximum rank drop (the number of positions that the most unfairly ordered element drops).}
	\vspace{-3mm}
	\label{tbl:results}
	\resizebox{1.01\columnwidth}{!}{%
		\centering\begin{tabular}{llcccccc}\toprule
			&        & \% Prot. &       & Ordering     & Rank & Selection \\
			& Method & output   & NDCG  & utility loss & drop & utility loss \\ \midrule
			D1 (51.2\%) & Color-blind & 25\% & 1.0000 & 0.0000 & 0 & 0.0000 \\
			COMPAS, & FA*IR p=0.5 & 46\% & \textbf{0.9858} & \textbf{0.2026} & \textbf{319} & \textbf{0.1087} \\
			race$=$Afr.-Am. & \citeauthor{Feldman2015} & 51\% & 0.9779 & 0.2281 & 393 & 0.1301 \\ \midrule
			
			D2 (80.7\%) & Color-blind & 73\% & 1.0000 & 0.0000 & 0 & 0.0000 \\
			COMPAS, & FA*IR p=0.8 & 77\% & \textbf{1.0000} & \textbf{0.1194} & \textbf{161} & \textbf{0.0320} \\
			gender$=$male & \citeauthor{Feldman2015} & 81\% & 0.9973 & 0.2090 & 294 & 0.0533 \\ \midrule
			
			D3 (19.3\%) & Color-blind & 28\% & 1.0000 & 0.0000 & 0 & 0.0000 \\
			COMPAS, & FA*IR p=0.2 & 28\% & \textbf{0.9999} & \textbf{0.2239} & \textbf{1} & \textbf{0.0000} \\
			gender$=$female & \citeauthor{Feldman2015} & 19\% & 0.9972 & 0.3028 & 278 & 0.0533 \\ \midrule
			
			D4 (69.0\%) & Color-blind & 74\% & 1.0000 & 0.0000 & 0 & 0.0000 \\
			Ger. cred, & FA*IR p=0.7 & 74\% & \textbf{1.0000} & \textbf{0.0000} & \textbf{0} & \textbf{0.0000} \\
			gender$=$female & \citeauthor{Feldman2015} & 69\% & 0.9988 & 0.1197 & 8 & 0.0224 \\ \midrule
			
			D5 (14.9\%) & Color-blind & 9\% & 1.0000 & 0.0000 & 0 & 0.0000 \\
			Ger. cred,& FA*IR p=0.2 & 15\% & \textbf{0.9983} & \textbf{0.0436} & \textbf{7} & \textbf{0.0462} \\
			age $<$ 25 & \citeauthor{Feldman2015} & 15\% & 0.9952 & 0.1656 & 8 & \textbf{0.0462} \\ \midrule
			
			D6 (54.8\%) & Color-blind & 24\% & 1.0000 & 0.0000 & 0 & 0.0000 \\
			Ger. cred, & FA*IR p=0.6 & 50\% & \textbf{0.9913} & \textbf{0.1137} & \textbf{30} & \textbf{0.0593} \\
			age $<$ 35 & \citeauthor{Feldman2015} & 55\% & 0.9853 & 0.2123 & 36 & 0.0633 \\ \midrule
			
			D7 (53.1\%) & Color-blind    & 49\% & 1.0000 & 0.0000 & 0 & 0.0000 \\
			SAT, 	    & FA*IR p=0.6    & 57\% & \textbf{0.9996} & \textbf{0.0167} & 365 & 0.0083 \\ 
			gender$=$female & \citeauthor{Feldman2015} & 56\% & \textbf{0.9996} & \textbf{0.0167} & \textbf{241} & \textbf{0.0042} \\ \midrule
			
			D8a (27.5\%)  & Color-blind    & 28\% & 1.0000 		& 0.0000 	& 0  & 0.0000 \\
			Economist,     & FA*IR p=0.3    & 28\% & \textbf{1.0000} & \textbf{0.0000} & \textbf{0}  & \textbf{0.0000} \\
			gender$=$female & \citeauthor{Feldman2015} & 28\% & 0.9935 		& 0.6109 	& 5 & \textbf{0.0000} \\ \midrule
			D8b (42.5\%)  & Color-blind    & 43\% & 1.0000 		& 0.0000 	& 0           & 0.0000 \\
			Mkt. Analyst,  & FA*IR p=0.4    & 43\% & \textbf{1.0000} & \textbf{0.0000} & \textbf{0}  & \textbf{0.0000} \\
			gender$=$male & \citeauthor{Feldman2015} & 43\% & 0.9422 		& 1.0000 	& 5 & \textbf{0.0000} \\ \midrule
			D8c (29.7\%)  & Color-blind    & 30\% & 1.0000 		& 0.0000 	& 0           & 0.0000 \\
			Copywriter,    & FA*IR p=0.3    & 30\% & \textbf{1.0000} & \textbf{0.0000} & \textbf{0}  & \textbf{0.0000} \\
			gender$=$female & \citeauthor{Feldman2015} & 30\% & 0.9782 & 0.4468 & 10 & \textbf{0.0000} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-3mm}
\end{table}

Table~\ref{tbl:results} summarizes the results. We report on the result using $p$ as a multiple of $0.1$ close to the proportion of protected elements in each dataset.
%
First, we observe that in general changes in utility with respect to the color-blind ranking are minor, as the utility is dominated by the top positions, which do not change dramatically.
%
Second, \algoFAIR achieves higher or equal selection utility than the baseline~\cite{Feldman2015} in all but one of the experimental conditions (D7).
%
Third, \algoFAIR achieves higher or equal ordering utility in all conditions. This is also reflected in the rank loss of the most unfairly
treated candidate included in the ranking ({\em i.e.}, the candidate that achieves the maximum ordering utility loss). %, which is always equal or less for \algoFAIR.

Interestingly, \algoFAIR allows to create rankings for multiple values of $p$, something that cannot be done directly with the baselines (\citet{Feldman2015} allows what they call a ``partial repair,'' but through an indirect parameter determining a mixture of the original and a transformed distribution).
%
Figure~\ref{fig:results-moving-p} shows results when varying $p$ in dataset D4 (German credit, the protected group is people under 25 years old).
%
This means that \algoFAIR allows a wide range of positive actions, for instance, offering favorable credit conditions to people with good credit rating, with a preference towards younger customers.
%
In this case, the figure shows that we can double the proportion of young people in the top-$k$ ranking (from the original 15\% up to 30\%) without introducing a large ordering utility loss and maintaining NDCG almost unchanged.

\begin{figure}[t!]
	\centering
	\subfloat[Ordering utility]{\includegraphics[width=.49\columnwidth]{pics/d4-protected-vs-ordering.png}}
	\subfloat[NDCG]{\includegraphics[width=.49\columnwidth]{pics/d4-protected-vs-ndcg.png}}
	\vspace{-2mm}
	\caption{Depiction of possible trade-offs using \algoFAIR. Increase in the percentage of protected candidates in D5 (German credit, protected group age $<$ 25) for increasing values of $p$, compared to %decrease in ordering utility (left) and decrease in NDCG (right).}
		ordering utility and NDCG.}
	\vspace{-\baselineskip}
	\label{fig:results-moving-p}
\end{figure}
