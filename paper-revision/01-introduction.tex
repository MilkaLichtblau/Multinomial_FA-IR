%!TEX root = main.tex
\section{Introduction}\label{sec:introduction}
Ranking is a fundamental task at the basis of several processes and services in online and offline world, including search, personalization, recommendation, and filtering.
%
The position that an item (or a person, as in our case of interest) receives in a ranking can have a substantial economic impact.
%
For instance, in a university admission only the top-$k$ ranked applicants will be admitted, so that those ones not making it into the top-$k$ will have zero utility.
%
In online search platforms, the position in a ranking influences to a large extent the attention that an item receives: only the top-$k$ items will be shown in the first results page (due to the physical limits of the screen), moreover it is well known that the users inspecting the results are susceptible to \emph{position bias}, i.e., they pay attention mostly to the top of the page and thus to the top-ranked items \cite{CraswellZTR08}.
%
As people search engines are increasingly common for job recruiting~\cite{raghavan2020mitigating} and even for finding companionship or friendship, top-$k$ ranking algorithms might have a direct and tangible impact on people's life.

% A top-$k$ ranking algorithm is typically used to find the most suitable way of ordering items (or persons, as in our case of interest), considering that if the number of people matching a query is large, most users will not scan the entire list.
%Conventionally, these lists are ranked in descending order of some measure of the relative fitness of items, according to the \emph{probability ranking principle}~\cite{robertson1977probability}.

The main concern motivating this work is that machine learning ranking algorithms may produce ranked lists that can systematically reduce the visibility of already disadvantaged groups~\cite{peder2008,Dwork2012} and legally protected categories such as people with disabilities, racial or ethnic minorities, or an under-represented gender in a specific professional domain.
%
\changed{This systematic bias may have various origins, including training data annotated by biased experts, click data from users exhibiting various kinds of biases, and even differences in document construction across groups (e.g., men and women complete different sections of their professional profiles differently, see \citet{altenburger2017there}).}
%
Furthermore, it is assumed that this bias manifests differently across groups, rendering inter-group relevance scores incomparable with each other.

According to \citet{friedman1996bias} a computer system is \emph{biased} ``if it systematically and unfairly discriminate[s] against certain individuals or groups of individuals in favor of others.
%
A system discriminates unfairly if it denies an opportunity or a good or if it assigns an undesirable outcome to an individual or a group of individuals on grounds that are unreasonable or inappropriate.''
%
Yet ``unfair discrimination alone does not give rise to bias unless it occurs systematically'' and ``systematic discrimination does not establish bias unless it is joined with an unfair outcome.''
%
In a ranking, the desired outcome for an individual is to be ranked as high as possible. In the cases in which there is a complete or big drop in utility after the $k$-th position, the desired outcome is to be ranked amongst the top-$k$. A ranking outcome can be deemed unfair, if members of one or more protected groups are systematically ranked below those of a privileged group.
%
%The ranking algorithm discriminates unfairly if this ranking decision is based fully or partially on the protected feature.
%%
%This discrimination is systematic when it is embodied in the algorithm's ranking model.
%
In this regard, the main issue is that machine learning models trained on datasets incorporating \textit{preexisting societal biases} will learn and structure such biases, eventually producing biased results, which once deployed in decision making in the real world, can reinforce existing biases and strengthen inequalities~\cite{oneil2016weapons}.
%

Based on this observation, in this article we study the problem of producing a ranking that we will consider fair given legally-protected attributes.
%
Here, we extend the fair top-$k$ ranking algorithm \algoFAIR ~\cite{zehlike2017fair} and its \emph{ranked group fairness criterion} to handle multiple protected groups.
%
Intuitively, given a set $G$ of different minority groups, our aim is to produce a ranking in which the proportion of each group $g \in G$ in any ranking prefix does not fall below given minimum proportions $p_G$, where $p_G$ is a vector containing the respective minimum proportion~$p_g$ per group $g$.
%
In this ranking, we also would like to preserve relevance/utility as much as possible.
%
A formal definition of the problem can be found in Section~\ref{sec:problem}.

The input vector $p_G$ of minimum proportions can originate from a legal mandate or from voluntary affirmative actions.
%
For instance, the US Equal Employment Opportunity Commission sets a goal of 12\% of workers with disabilities in federal agencies in the US,\footnote{US EEOC: \url{https://www1.eeoc.gov/eeoc/newsroom/release/1-3-17.cfm}, Jan 2017.}
%
while in Spain, a minimum of 40\% of political candidates in voting districts exceeding a certain size must be women~\cite{verge2010gendering}.
%
In other cases, such quotas might be adopted voluntarily, for instance through a diversity charter.\footnote{European Commission: \url{http://ec.europa.eu/justice/discrimination/diversity/charters/}}
%
In general these measures do not mandate perfect parity, as distributions of qualifications across groups can be unbalanced for legitimate, explainable reasons~\cite{zliobaite2011handling,pedreschi2009integrating}. % pedreschi2009integrating has the truck driver license example }
%
The ranked group fairness criterion compares the number of protected elements in every prefix of the ranking with the expected number of protected elements if they were picked at random using a multinomially distributed statistical process (``dice rolls'' with each side $g$ of the dice representing a group, and having success probability $p_g \in p_G$).
%
Given that we use a statistical test for this comparison, we include a significance parameter $\alpha$ corresponding to the probability of a type-I-error, which means rejecting a fair ranking.
%

\paragraph{Example.} In the following we want to illustrate why we have to extend the Fair Top-$k$ Ranking Problem from~\citet{zehlike2017fair} to be based on a multinomial distribution, instead of just applying binomial \algoFAIR to a ranking for each group consecutively.
%
Consider the rankings in Table~\ref{tbl:multinomial_intro_example} corresponding to a number of top-10 rankings of applicants in a credit approval process.
%
The rankings are obtained based on the credit worthiness of each applicant taking into account different features such as account status, credit duration, and credit amount.
%
We also present to which protected group each individual belongs based on their demographics ``age'' and ``race.''
%
In this example we consider people as protected, if they are either young, or black, or both.
%
Suppose we have two protected groups, namely young white candidates and old black candidates, and one non-protected group, old white candidates (for now, we do not consider young black candidates on purpose).

\begin{table}[t]
	\caption[Introductory example for the extension of \algoFAIR to multiple protected groups]{Five possible rankings of top-10 results involving people belonging to two age groups ``y'' and ``o'' and two racial groups ``w'' and ``b''.
		%
		Suppose the protected groups only contain three young white (red) and three old black persons (blue), and that we set the same minimum proportions for these two groups $\minprop_{\text{young}} = \minprop_{\text{black}} = 0.3$.
		%
		Using binomial \algoFAIR~\cite{zehlike2017fair}, this translates into having at least one protected person in the first seven positions.
		%
		The first ranking shows the colorblind ranking, which is unfair for both groups (note that the remaining protected candidates have been ranked outside of the top-10 and are not shown).
		%
		The second ranking would be considered fair for young people, but not for black people; the third ranking would be considered fair for black people, but not for young people; the fourth ranking combines black and young people into one protected group, but because the group-specific bias is higher for young people, they are ranked below all black candidates; the fifth ranking was created using the multinomial extension of \algoFAIR and would satisfy the multinomial ranked group fairness condition for both young and black people.
		%
		Note that this is only one fair ranking out of many possibilities.
		\label{tbl:multinomial_intro_example}}
	\centering\begin{tabular}{lcccccccccc}\toprule
		Position & 1 &2 & 3 &4& 5& 6& 7& 8& 9& 10  \\
		\midrule
		\rowcolor[HTML]{C0C0C0}
		Colorblind & o/w & o/w & o/w & o/w & o/w & o/w & o/w & o/w & \textcolor{blue}{o/b} & \textcolor{red}{y/w} \\
		Young $\minprop_\textit{y}=0.3$ & o/w &o/w& o/w& o/w& o/w& o/w& \textcolor{red}{y/w}& o/w & o/w & \textcolor{blue}{o/b} \\
		\rowcolor[HTML]{C0C0C0}
		Black $\minprop_b=0.3$ & o/w &o/w &o/w& o/w &o/w &o/w& \textcolor{blue}{o/b}& o/w& o/w& \textcolor{red}{y/w} \\
		Young and Black $p_{\textit{yb}}=0.6$& o/w &o/w& \textcolor{blue}{o/b} & o/w& \textcolor{blue}{o/b}& o/w & \textcolor{blue}{o/b}& o/w& \textcolor{red}{y/w}& o/w \\
		\algoFAIR from~\cite{zehlike2017fair} &&&&&&&&&&\\
		\rowcolor[HTML]{C0C0C0}
		Young and Black $\minprop_{\textit{y}} = \minprop_{\textit{b}} = 0.3$& o/w& o/w &\textcolor{red}{y/w}& \textcolor{blue}{o/b}& o/w& \textcolor{red}{y/w} &\textcolor{blue}{o/b}& o/w& \textcolor{red}{y/w}& \textcolor{blue}{o/b} \\
		\rowcolor[HTML]{C0C0C0}
		multinomial \algoFAIR &&&&&&&&&&\\
		\bottomrule
	\end{tabular}
\end{table}

Considering the definition of \textit{ranked group fairness} for one protected group (Definition~3.1 and~3.2 in~\cite{zehlike2017fair}) with minimum proportion $\minprop=0.3$ and significance $\alpha=0.1$, these parameters translate into requiring at least one protected candidate in the top-7 positions of a ranking with ten positions in total (see Table 2 in~\cite{zehlike2017fair}).
%
Suppose we set the same minimum proportions for both protected groups $\minprop_{\text{young}} = \minprop_{\text{black}} = 0.3$ and we assume that the colorblind ranking (upper most ranking in Table~\ref{tbl:multinomial_intro_example}) does not meet ranked group fairness with these requirements.
%
The next two rankings have been obtained using \algoFAIR for one protected group, each focusing either on the protected group of old blacks or young whites, while treating the rest as non-protected.
%
We see directly why this is problematic: equal minimum proportions $\minprop$ result in exactly the same requirements of protected candidates for each group.
%
It is not clear whether a black or a young candidate should be preferred at position 7.
%
More importantly, it is also not clear how to proceed with the one, that is not chosen.
%
Suppose the young candidate is chosen to be ranked to position 7.
%
As old black candidates are not considered as protected by this mTable, then they would be ranked even lower than their original position in the colorblind ranking (observe the second ranking in Table~\ref{tbl:multinomial_intro_example}).

To overcome this, suppose we aggregated all protected groups together into one big protected group and add up the minimum proportions to one that fits the total share of protected candidates.
%
Following our example, we combine the group of young whites and old blacks into one group with minimum proportion $\minprop=0.6$.
%
This however, neglects to account for bias that manifests differently across protected groups.
%
Following our example, if the bias in credit scores of young people is significantly higher than for black people, then all young whites will be ranked below the old blacks in the fair ranking (observe the fourth ranking in Table~\ref{tbl:multinomial_intro_example}).
%
Additionally, combining all protected groups into one group ignores the problem of \emph{intersectional discrimination}.
%
Intersectional discrimination refers to the fact that personal, political, and social identities of an individual can be combined to form a unique profile that can be discriminated.
%
If we additionally considered young black people as a third protected group, the bias in their scores would manifest itself through two dimensions of discrimination, namely through age \emph{and} race, which commonly leads to even more bias in their scores, and thus potentially to the lowest positions, even when applying \algoFAIR.
%
Therefore young black candidates should be considered as their own group whose scores are not comparable with the ones of young whites, or old blacks.

From the above we conclude that providing a fairness definition for rankings that either treats protected groups consecutively, or assembles all groups into one big group, can not guarantee a fair ranking for multiple protected groups with different bias manifestations.
%
This highlights the need for a ranked group fairness notion for multiple protected groups that is based on a multinomial distribution.
%
The fifth ranking in Table~\ref{tbl:multinomial_intro_example} is an example of a multinomially fair top-10 ranking for the protected groups young and black.
%
It is created using our \textit{multinomial ranked group fairness criteria} and the multinomial \algoFAIR algorithm proposed in this paper.
%
Note that this is only one fair ranking out of many possibilities.
%
We remark that our method is suitable for concerns about intersectional discrimination, by forming groups through the Cartesian product of protected attributes.

\subsection{Contributions and Roadmap}
We define and analyze the {\sc Fair Top-$k$ Ranking problem} with multiple protected groups, in which we want to determine a subset of $k$ candidates from a large pool of $n \gg k$ candidates, in a way that maintains high utility (selects the ``best'' candidates from each group), subject to a group fairness criterion.
%
This work addresses multiple protected groups extending our previous conference paper~\cite{zehlike2017fair} that only considered the single protected group case.
%
%The running example we use in this paper is that of selecting automatically, from a large pool of potential candidates, a smaller group that will be interviewed for a position.
%

Our notion of utility assumes that we want to invite the most qualified candidates from each group, while their qualification is equal to a relevance score calculated by a ranking algorithm.
%
This score is assumed to be based on relevant metrics for evaluating candidates for a work position, which depending on the specific skills required for the job could be their grades ({\em e.g.}, Grade Point Average), their results in a standardized knowledge/skills test specific for a job, such as their typing speed in words per minute for typists, or their number of hours of flight in the case of pilots.
%
We note that this measurement will embody \emph{preexisting bias} ({\em e.g.}, if black pilots are given less opportunities to flight they accumulate less flight hours), as well as \emph{technical bias}, as learning algorithms are known to be susceptible to direct and indirect discrimination~\cite{tuto2016,HajianFerrer12}.
%
We furthermore note that different manifestations of such bias exist for each group and are usually stronger for intersectional groups, that is, the preexisting bias against black women is stronger than the one for women or blacks in general.
%

Our utility principle is operationalized in two ways.
%
First, we prefer rankings in which every individual in the top-$k$ is more qualified than every candidate not included in the top-$k$ (or in which the difference in their qualifications is small). We call this criterion \emph{selection utility}.
%
Second, we prefer rankings in which for every pair of individuals included in the top-$k$, either the more qualified candidate is ranked above, or the difference in their qualifications is small. We call this criterion \emph{ordering utility}.
%
Note however, that in a setting with multiple protected groups, optimal selection and ordering utility cannot be guaranteed because of said differences in the group skews.
%
Mathematically this means that the optimal solution for multinomial ranked group fairness ({\em i.e.}, for more than one protected group) is a solution space rather than just a single point as it was in~\cite{zehlike2017fair}, while the optimal solution in terms of utility is still a single point within said solution space whose location depends on the candidate set at hand.
%
We want to stress that trying to find this point of optimal utility corresponds to a worldview in which one assumes that utility measures of candidates across different groups are actually comparable and that the per-group bias is known a-priori.
%
We believe that the group skew unawareness is a necessary condition for the justification of post-processing algorithms in general and we therefore explicitly do not search for the optimal solution in terms of utility.
%
We will go into more depth on this in Section~\ref{subsec:individual-fairness}.
%

Our definition of \emph{ranked group fairness} reflects the legal principle of group under-representation in obtaining a benefit \cite{ellis2012eu,lerner2003group}.
%
%We use the standard notion of protected groups ({\em e.g.}, ``people with disabilities''); where protection emanates from a legal mandate or a voluntary commitment.
%
%The group under-representation principle, and the related disparate impact doctrine~\cite{Barocas2014} addresses the fact that there might be differences in qualification across different groups by \emph{not} mandating an equal proportion of candidates from the protected group and non-protected group in the output. It simply states that the proportions cannot be too different.
%
We formulate a criterion by applying a statistical test on the proportion of protected candidates on every prefix of the ranking, which should be indistinguishable or above a given minimum.
%
%This procedure can be seen as a form of positive action to ensure that the proportion of protected candidates in every prefix of the top-$k$ ranking is statistically indistinguishable from a policy target.
%
We also show that the verification of the ranked group fairness criterion can be implemented efficiently \removed{by} \changed{after} pre-computing a verification data structure that we call \emph{mTree}.
%
This tree contains all possibilities to create a ranking that satisfies ranked group fairness and we provide an algorithm to build and persist it.
%
We also provide an algorithm \algoCorrect for the mTree adjustment due to multiple dependent hypothesis testing, where we compute $\alphaadj$ such that the type-I-error is less or equal to $\alpha$ for the \emph{entire tree}.
%

Finally, we propose an \removed{efficient} algorithm, named \algoFAIR, for producing a top-$k$ ranking that maintains high utility while satisfying ranked group fairness, as long as there are ``enough'' protected candidates from each group to achieve the desired minimum proportions.
%
Note that if a group of protected candidates is too small to satisfy ranked group fairness, the ranking is necessarily bound to under-represent them.
%
We also present extensive experiments to evaluate the performance of our approach compared to a group-unaware method (the so-called ``color-blind'' method) with respect to both, the expected utility of a ranking and the fairness degree measured, in terms of expected exposure.

Summarizing, the main contributions of this paper are:
\begin{compactenum}
	\item the principled definition of \emph{ranked group fairness} for multiple protected groups, and the associated  {\sc Fair Top-$k$ Ranking problem};
	\item the \algoFAIR\ algorithm for multiple protected groups to produce a top-$k$ ranking that maximizes utility while satisfying ranked group fairness.
	\item a mathematical framework to solve the problem of multiple \emph{dependent} hypotheses testing: we will see how to adjust the given significance level $\alpha$ to a corrected $\alphaadj$, such that the \emph{overall} probability for a type-1-error equals the given significance level $\alpha$.
	\item the algorithms \algoComputeMTree and \algoCorrect which translate ranked group fairness into a pre-computed data structure. This will enable efficient verification of ranked group fairness at testing time and efficient creation of fair rankings \changed{at creation time}.
\end{compactenum}

Our method can be used within an anti-discrimination framework such as \emph{affirmative actions}~\cite{sowell2005affirmative}.
%
We do not claim these are the only way of achieving fairness, but we provide \emph{an algorithm grounded in statistical tests that enables the implementation of a positive action policy in the context of ranking}.

The rest of this paper is organized as follows.
%
The next section presents a brief survey of related literature,
%
while Section~\ref{sec:problem} introduces our ranked group fairness and utility criteria and a formal problem statement.
%
Section~\ref{sec:fairness-verification} presents a data structure (mTree) that allows fast verification of the ranked group fairness criterion.
%
Section~\ref{sec:model-adjustment} presents a procedure for statistical test significance adjustment, which is required due to multiple hypotheses testing.
%
Section~\ref{sec:multinom-fair-algo} describes the \algoFAIR\ algorithm for fair rankings.
%
Section~\ref{sec:experiments} presents experimental results.
%
Section~\ref{sec:conclusions} presents our conclusions and future work.
