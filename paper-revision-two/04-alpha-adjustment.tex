%!TEX root = main.tex

\section{Ranked Group Fairness Verification}
\label{sec:fairness-verification}
To verify ranked group fairness efficiently in time $O(k)$, a pre-computed data structure can be used, which is obtained by the \emph{inverse multinomial CDF} with parameters $k, p_G$ and $ \alpha $.
%
The inverse CDF takes as input a probability (in our case multiple probabilities $p_G$) and returns that value of a random variable, at which the probability of said random variable being less than or equal to the returned value equals the input probability.
%
As the multinomial CDF is not injective and hence has no inverse, there is no quantile function that tells us exactly how many candidates are needed at each $ k $.
%
Instead, there are various manifestations of $ \tau_G $ that satisfy the fair representation condition $F(\tau_G;k,p_G) > \alpha$, which is why the verification data structure has the shape of a tree for each $ p_G, k $ and $ \alpha $.
%
\input{pics/mTrees}
Figure~\ref{fig:mtree-symmetric-unadjusted} shows an example of such tree with $p_G = \langle 1/3, 1/3 \rangle$.
%
Each tree level corresponds to the $k$-th position in the ranking and are to be read from left to right, {\it i.e.}, the root level corresponds to the first ranking position and so forth.
%
As an example, consider tree level 4 in Fig.~\ref{fig:mtree-symmetric-unadjusted}.
%
At this level, we have three nodes ``(4, [2,0]),'' ``(4, [1,1]),'' and ``(4, [0,2])'' each of them containing a set of minima for elements of the protected groups (the nodes do not include any minima for the non-protected group).
%
This means it is acceptable to have among the first four elements in the ranking either at least 2 elements from protected group 1, or at least 1 element from each protected group, or at least 2 elements from protected group 2.
%
Note however, that nodes have parental relationships and that each \emph{path} corresponds to a fair distribution of protected candidates in the ranking.
%
Thus, if at level 4 we rank two candidates from protected group 1, thus satisfying the node with the asterisk in Fig.~\ref{fig:mtree-symmetric-unadjusted}, at level 5 have to satisfy either node ``(5, [2,1])'' or node ``(5, [3,0]).''
%
The other nodes ``(5, [1,1]),'' ``(5, [1,2]),'' and ``(5, [0,3])'' are not a child of node ``(4, [2,0])'' and therefore cannot be considered to satisfy the ranked group fairness condition anymore.
%
The tree is symmetric when the minimum proportions $p_g \in p_G$ are equal, and asymmetric when they are different.
%
We note that it is possible to trade-off under-representation in one protected group against over-representation in another protected group, but only up to a point: the representation of every protected group cannot fall below a certain minimum. Additionally, when using this method to construct a ranking, we choose the most likely path, not any path, and thus avoid extremely unbalanced situations.
%
%Figure~\ref{fig:mtree-asymmetric-unadjusted} show the tree is asymmetric when the minimum proportions are different, in that example $0.2$ and $0.4$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ALGORITHM COMPUTE MTREE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{algos/algo-computeTree.tex}

As stated at the beginning of this section mTrees are pre-computed data structures that allow efficient verification of the ranked group fairness condition.
%
To construct them we use Algorithms~\ref{alg:computeMTree} and~\ref{alg:imcdf}.
%
The first algorithm \algoComputeMTree takes a triple $(k,p_G,\alphaadj)$ as input and returns the mTree for these parameters.
%
First it creates an auxiliary root node that contains $|G|$ zero entries, which serves as the root parent.
%
Then, for each parent node \texttt{parent} at each position position $i \leq k$ it calls the inverse multinomial CDF function.
%
The second algorithm \algoImcdf takes as input a 4-tuple $(\alphaadj, i, p_G, \texttt{parent})$ and returns all nodes satisfying the following two conditions:
\begin{inparaenum}[(i)]
	\item they have to be children of node \texttt{parent}, and
	\item they satisfy the fair representation condition (Def.~\ref{def:fair-representation-condition}).
\end{inparaenum}

\section{Model Adjustment}
\label{sec:model-adjustment}

Recall that we restrict the probability of a type-I-error in our ranked group fairness test to $\alpha$ (Def.~\ref{def:fair-representation-condition}).
%
To verify ranked group fairness, we test if a ranking satisfies the \emph{fair representation condition} (Def.~\ref{def:fair-representation-condition}) for each prefix  of size $1, 2, \dots, k$.
%
Because of this multiple hypotheses testing, if we perform each test with significance $\alphaadj = \alpha$, we risk rejecting fair rankings at a rate larger than $\alpha$.
%
Hence, we require an adjusted significance $\alphaadj = \adj(\alpha, k, p)$ for each fair representation test.
%

%The process of determining the adjusted significance $\alphaadj$ is done by assuming that the probabilistic model for generating rankings presented by~\citet{yang2016measuring} creates fair rankings, and hence our test should accept those rankings with probability $1 - \alphaadj$.
%%
%In that generative model, we rank items from each group separately, and then merge lists by iteratively selecting the highest ranked candidate from protected group $g$ with probability $p_g$, or the highest ranked candidate from the non-protected group with probability $p_0 = 1-\sum_{j=1}^{|G|} p_j$.
%

\begin{figure}[t!]
	\centering
	{\includegraphics[width=.48\textwidth]{pics/failProbPlotMultinom.png}}
	\CaptionMargin
	\caption{Experiments on data generated by a simulation, showing the need for multiple tests correction.
		%
		The data has two protected groups, rankings of lengths $k$ are created by the stochastic process of a multinomial distribution (``rolling a 3-sided dice'') with $p_G = (0.33, 0.33)$.
		%
		These rankings should have been rejected as unfair at a rate $\alpha = 0.1$.
		%
		However, we see that the rejection probability increases with $k$.
		%
		Note the scale of $k$ is logarithmic.}
	\label{fig:why-adjustment-is-needed-multinomial}
\end{figure}

Figure~\ref{fig:why-adjustment-is-needed-multinomial} illustrates what happens, if we do correct the significance level $\alpha$.
%
In the figure, we assume there are two protected groups with $p_1=p_2=\frac{1}{3}$, and we generate inherently fair rankings of various lengths from $k=5$ to $k=5,000$.
%
We test each ranking for fair representation with $\alphaadj = \alpha$ and plot the probability of a type-I-error (declaring this fair ranking as unfair).
%
We observe that the probability of rejecting a fair ranking as unfair increases with the number of hypotheses tests, i.e. with $k$.

If each of the $k$ tests performed on a ranking were independent, we could use {\v S}id{\'a}k's correction, and set $\alphaadj = 1 - (1 - \alpha)^{1/k}$.
%
However, the tests are not independent, as the tests with prefixes $k_1, k_2$ (for instance), overlap on their first $\min(k_1, k_2)$ elements.
%
In this section, we present a procedure to adjust the significance level under dependent hypotheses tests, such that an mTree has an overall probability for a type-I-error of $\alpha$.

\subsection{General Procedure}
\label{subsec:general-process}
%
With the presence of more than one protected group, the analytical extension of the model adjustment to a multinomial setting is too complex to be written into a closed formula.
%
In contrast to the model adjustment for one protected group which we used in~\cite{zehlike2017fair},
%(see also Section~\ref{sec:adjustment-binomial})
we found no analytical way to calculate all permutations which pass or fail the test.
%
Therefore, we develop an empirical procedure to adjust $ \alpha $:
%
\begin{enumerate}
	\item Get input $ p_G, k, \alpha $.
	\item Build mTree with input $ \alphaadj = \alpha $.
	\item Create $M$ rankings by rolling a biased $ |G| $-sided dice with each side's probability to show corresponding to a minimum proportion in vector $ p_G $.
	\item Test all those rankings against the mTree and count how many tests fail.
	%
	Remember that we want to observe a maximum failure probability of $ \failprob=\alpha $, because all rankings created by this multinomial stochastic process are considered to be inherently fair.
	\item If $ \failprob > \alpha $, we choose a new $ \alphaadj $ using a binary search heuristic.
	\item Now we build a new mTree using $ \alphaadj $ and repeat the procedure until $ \failprob \approx \alpha $.
\end{enumerate}
%
Once we found $\alphaadj$ we can recompute the mTree from Figure~\ref{fig:mtree-symmetric-unadjusted} %and~\ref{fig:mtree-asymmetric-unadjusted}
 to obtain an overall significance level of $\alpha = 0.1$.
%
Figure~\ref{fig:mtree-symmetric-adjusted}
%and~\ref{fig:mtree-asymmetric-adjusted}
shows the adjusted mTree with the same parameters $p_G$.
\input{pics/mTreesAdjusted.tex}

\subsection{Optimizations for the MTree Calculation}
\label{subsec:mtree-optimization}
In this subsection we explain how we optimize the adjustment procedure from the previous section to reduce complexity.
%
At each iteration, it requires to compute a new mTree, which is expensive on its own.
%
Furthermore, depending on the level of accuracy needed, a large number of iterations $M$ might be required and the binary search heuristic may need many steps to find $\alpha$, if large intervals have to be searched.
%
We use three strategies to drastically reduce computational costs of this simulation-based adjustment:
%
\begin{inparaenum}[(i)]
%
\item we reduce the space requirements of the mTree structure;
%
\item we exploit the monotonicity of $\alphaadj$ with respect to $\alpha$ and use a regression procedure to speed up the binary search.
%
\item we apply the adjustment procedure for small trees first and only increase $k$, if we found the correct  $\alphaadj$ for the small trees.
\end{inparaenum}

\subsubsection{Reducing mTree Space Requirements.}
\label{subsubsec:reducing-space-requirements}
We improve the mTree data structure by excluding the calculation and storage of redundant information.
%
First, we store each node only once at each level and duplicate nodes are combined into a single node with multiple parents.

Second, in case of equal minimum proportions for all groups $p_1 = p_2 = \ldots = p_{|G|}$ the mTree shows a convenient property that we can use to reduce additional space, as well as computation time.
%
Remember that whenever the multinomial CDF value falls below $\alpha$ for a particular position $i$, we have to put a protected candidate onto $i$.
%
For equal minimum proportions the tree branches into $|G| - 1$ symmetric nodes $m(i)$ of the same likelihood.
%
As an example reconsider the mTree from Figure~\ref{fig:mtree-symmetric-adjusted} at level 6.
%
For two protected groups with minimum proportions $[1/3, 1/3]$ we see that the tree branches into two symmetric nodes $(6, [1, 0])$ and $(6, [0,1])$.
%
Both have the same multinomial CDF values.
%
We store only one of the nodes and flag it as ``has mirrored node'' and continue our mTree computation only in the stored branch.
%
This way we save half of the space and computation time needed, without losing any information about the tree.

Furthermore, we reduce the size of the mTree by actually leaving out the parent-child relationship and merely storing the nodes itself together with their respective depth levels.
%
Figure~\ref{fig:mtree-symmetric-adjusted} %and~\ref{fig:mtree-asymmetric-adjusted}
shows the mTree structure with all parent-child relations as edges.
%
We leave out these edges, thus reducing space, because we can prove that, if a single node exists on each level, which accepts a given ranking as fair, then a valid path to that node exists in the tree.

Before we can prove this property, we need to introduce the following definition, which formalizes what a successful mTree test at level $i$ looks like.
%
\begin{definition}[Successful mTree Testing]
\label{def:valid-mtree-test}
Let $\tau$ be a ranking of size $k$ and $\tau_{G,i}=(\tau_{1},\ldots,\tau_{|G|})$ the numbers of ranked protected elements from group $1,\ldots,|G|$ up to position $i$.
%
Let us assume $MT$ be a $mTree$, with node $MT_{G,i}=[m_1(i),\ldots,m_{|G|}(i)]$ giving the number of protected candidates of group $1,\ldots,|G|$ required up to position $i$.
%
We write $\tau_{G,i} \geq m_{G,i}$ if $\tau_g \geq m_g$ for all $g=1,\ldots,|G|$.
%
We call a test on level $i$ of $MT$ successful, iff $\tau_{G,i} \geq m_{G,i}$.
\end{definition}
%
Again, in order to remove the parental relationships in the mTree, thus reducing storage space, we have to prove that, if we test a ranking on each level of the mTree successfully, the entire ranking will be fair according to the ranked group fairness definition.
%
We prove this by showing that, if a ranking passes the test for any two nodes $n_1$ and $n_2$ at two consecutive levels $h$ and $h+1$, and $n_1$ is \emph{not} a parent of $n_2$, then all actual children of $n_1$ will have a weaker requirement than $n_2$ and will hence also test successfully.
%
Moreover, we show that all nodes at level $h+1$ for which the ranking fails the test are part of a path that already rejected it as unfair at level $h$.
%
Consider an example from Figure~\ref{fig:mtree-symmetric-adjusted} : Let us assume a ranking passes the test at level $8$ with exactly the required protected items $(8, [1,1])$.
%
Now let's assume that at level $9$, the given ranking would pass the test for node $(9, [2,1])$, which is not a successor of $(9, [1,1])$.
%
In fact we see that the actual successor of $(8, [1,1])$ is a node with the same requirements $(9, [1,1])$.
%
However, if our ranking passes the test for the stricter node $(9, [2,1])$, it also passes for $(9, [1,1])$ and thus we do not need to know the true parent of $(9, [2,1])$.
%
Note that the ranking would fail at node $(9, [3,0])$, but with $\tau_G = (1,1)$ at level 8 it would have failed already at $(9, [3,0])$'s predecessor $(8, [2,0])$.
%
\begin{theorem}
\label{theorem:lazy-mTree-test}
Let $MT$ be a mTree and $\tau$ a ranking of size $k$.
%
There exists at least one successful test for $\tau$ at each level of $MT$,
iff there exists a path from the root of $MT$ to a leaf of $MT$ and $\tau$ passes the test at each node on that path.
\end{theorem}
%
\begin{proof}
	\label{proof:lazy-mTree-test}
	$\;$\\
\paragraph{"$\Leftarrow$"}:
If there exists a path for which $\tau$ passes the test at each node, there also exists a node on each level of $MT$ where $\tau$ passes the test because every path has a node on every level.

\paragraph{"$\Rightarrow$"}:
	Let $MT$ be a mTree and $\tau$ be a ranking that passes the test at level $h$ of $MT$.
	%
	Let $m_{G,h}=[m_{1}(h), \ldots, m_{|G|}(h)]$ be the node on level $h$ that successfully tested $\tau$.
	%
	Let further be $\tau_g(h)$ the number of protected candidates of group $g$ ranked at up to position $h$.
	%
	Let $\sum_{g=1}^{|G|} |m_{g}(h) - \tau_{g}(h)| = 0$, meaning that the ranking includes the exact amount of required protected candidates at level $h$ and not more. 
	%
	Note that this assumption reduces the number of nodes at which $\tau$ passes the test at level $h$ to a minimum.
	%
	If we assumed that $\tau$ included more protected candidates than required, we would only increase the number of nodes at level $h$ for which $\tau$ would pass the test.
	%
	Thus, we can proof this direction with the above assumption without loss of generality.
	%
	Let $m_{G,(h+1)}$ be a node which tests $\tau$ successfully on level $h+1$ with $\sum_{g=1}^{|G|} |m_{g}(h+1) - \tau_{g}(h+1)| = 0$.
	%
	For all entries $m_{g}(h+1)$ of $m_{G,(h+1)}$ it is that $m_{g}(h) \leq m_{g}(h+1)$ by construction of the mTree, in which requirements for protected candidates can only increase or stay the same, but cannot decrease.

	\noindent We can now distinguish between the following two cases:
	\\
	\textbf{Case 1:} $m_{G,(h+1)}$ is a child of $m_{G,h}$.
	%
	Then they form a path. %UNNECESSARY:% Note that $\sum_{g=1}^{|G|} (m_{g}(h+1) - m_{g}(h) \leq 1)$.
	\\
	\textbf{Case 2:} $m_{G,(h+1)}$ is not a child of $m_{G,h}$.
	%
	Let now ${m'}_{G,(h+1)}$ be a child of $m_{G,(h)}$.
	%
	Because of $\sum_{g=1}^{|G|} |m_{g}(h) - \tau_{g}(h)| = 0$, and a successful test at level $h+1$, the following inequations hold: $\sum_{g=1}^{|G|} |m_{g}(h) - m_g(h+1)| \leq 1$ and $\sum_{g=1}^{|G|} |m_{g}(h) - m'_g(h+1)| \leq 1$.
	%
	If $\sum_{g=1}^{|G|} |m_{g}(h) - m_g(h+1)| = \sum_{g=1}^{|G|} |m_{g}(h) - m'_g(h+1)| = 0$ or $\sum_{g=1}^{|G|} |m_{g}(h) - m_g(h+1)| = \sum_{g=1}^{|G|} |m_{g}(h) - m'_g(h+1)| = 1$, it follows that $m_{G,(h+1)} = {m'}_{G,(h+1)}$ and we would have a contradiction with the fact that ${m'}_{G,(h+1)}$ is not a child of $m_{G,h}$, because then the nodes would be equal or different by one unit in one position.

	Hence, we need that either
	(2.1) $\sum_{g=1}^{|G|} |m_{g}(h) - m_g(h+1)| = 1$ and $\sum_{g=1}^{|G|} |m_{g}(h) - m'_g(h+1)| = 0$,
	or
	(2.2) $\sum_{g=1}^{|G|} |m_{g}(h) - m_g(h+1)| = 0$ and $\sum_{g=1}^{|G|} |m_{g}(h) - m'_g(h+1)| = 1$.
	%
	Case (2.1) means because of $\sum_{g=1}^{|G|} |m_{g}(h) - m_g(h+1)| = 1 > \sum_{g=1}^{|G|} |m_{g}(h) - m'_g(h+1)| = 0$ that the mTree accepts a ranking with one more protected candidate than required by the actual child of $m_{G,h}$, which contradicts our hypothesis that the ranking included the exact amount of required protected candidates. It follows that if we test $\tau_g (h+1)$ successfully with $m_{G,h+1}$ it would also satisfy ${m'}_{G,h+1}$.
Case (2.2) is impossible according to algorithm \ref{alg:imcdf}.
In detail, if $\sum_{g=1}^{|G|} |m_{g}(h) - m_g(h+1)| = 0$ it means that $ m_g(h+1) = m_{g}(h)$ and therefore, $F(m_{g}(h);h+1,p_G) > \alpha$ (line $4$ of algorithm \ref{alg:imcdf}). But if for the child of $m_{G,h}$, namely ${m'}_{G,h+1}$ it holds that $\sum_{g=1}^{|G|} |m_{g}(h) - m'_g(h+1)| = 1$, it means that
$F(m_{g}(h);h+1,p_G) \leq \alpha$ so that we would have added a new node as a child of $m_{g}(h)$ according to lines  $8-14$ of algorithm \ref{alg:imcdf}. Since both conditions cannot be true at the same time, Case (2.2) cannot occur.
\\
In summary, Case (2.1) will only occur if we ranked more protected candidates than needed, and that the resulting test would be stricter than following a path through the mTree.
%
We showed that Case (2.2) is impossible. There is only Case 1 left if we have ranked exactly the number of protected candidates needed at each level of the mTree.
%
It follows that for any ranking that is tested successfully on each level of the mTree, it either was tested by nodes of a path through the mTree or was tested by a series of nodes which is stricter than such path.
\end{proof}
%
\noindent Because of Theorem~\ref{theorem:lazy-mTree-test} we do not need to keep the tree structure (i.e. parental relationship between nodes) and may store only a set of nodes for each level, while removing duplicate entries.

%
\begin{figure}[t!]
	\centering
	%
	\subfloat[Training data $R$ for a regression model to predict a good candidate for $\alphaadj$. Each pair $(k_j, \alpha_{c_j})$ is computed for small $k$ using the procedure described in Subsection~\ref{subsec:general-process}. \label{fig:regression-training-data}]
	{\includegraphics[width=.48\textwidth]{pics/alpha_030303_01.png}}\hfill
	%
	\subfloat[Computation time comparison between binary search only and binary search combined with regression for the multinomial significance adjustment.  \label{fig:regression-time-saved}]
	{\includegraphics[width=.48\textwidth]{pics/computationTimeRegressionVSBinaryMultinomial.png}}\hfill
	\tablemargin
	\caption{}
	\label{fig:regression_adjustment_benefits}
\end{figure}
\subsubsection{Finding a Good Starting Candidate for the Binary Search}
We use a second-degree polynomial regression model to get our first estimate for a good $\alphaadj$ candidate and apply the binary search heuristic from that candidate, rather than starting with a random value, that might be very far away from the correct $\alphaadj$.
%
We need a few additional parameters as input: \texttt{kTarget} -- the length of the target ranking, \texttt{kStart} -- the size of the first mTree, \texttt{maxPreAdjustK} -- the maximum size of the mTree before we use regression to predict a good candidate $\alpha_{c_r}$ for the final $\alphaadj$, and \texttt{num\_iterations} -- the number of training instances to be computed.
%
To create a training dataset $R$, we compute \texttt{num\_iterations} small mTrees (i.e. with different $k \leq $ \texttt{maxPreAdjustK}) and adjust the respective $\alpha$ values as described in Subsection~\ref{subsec:general-process}.
%
For each iteration $j$ the pair $(k_j, \alpha_{c_j})$ is stored as a training instance in $R$.
%
Figure~\ref{fig:regression-training-data} shows a training set for $p_G=[1/3, 1/3], \texttt{maxPreAdjustK}=100$.
%
Then a regression model is trained to predict $\alpha_{c_r}$ for \texttt{kTarget}.
%
This $\alpha_{c_r}$ is now used to start the binary search for the correct and final $\alphaadj$.
%
Figure~\ref{fig:regression-time-saved} shows the runtime difference for the model adjustment routine with and without the use of regression.

\subsubsection{Adjusting for Small $k$ First}
We use the fact that given a value of $\alpha$, the mTree calculation is not dependent on $ k $, i.e., a mTree for $k=20$ and a mTree for $k=10$ for the same $\alpha$ are equal in the first ten positions.
%
This means that we can start the adjustment from the root node and expand the tree gradually to find the correct $ \alphaadj $, because if $\failprob$ is too high for given $k$, it will also be too high for any $k' > k$. %in particular and we do not have to consider the larger tree, as long as we do not have a good $\alphaadj$ for $k=10$.
%
%In order to show that we can indeed do that, assume that the for loop of algorithm \ref{alg:computeMTree} stops at $k=10$ instead of $k=20$.
%
%Furthermore, we can understand the mTree as a set of rules that a ranking has to satisfy.
%
%If there are no rules for how many protected candidates are required after position $10$, this is equivalent to not requiring more protected candidates after position $10$.
%
%Thus, the probability that a fair ranking is rejected by a shorter mTree is less or equal to a deeper mTree.
%
%We utilize this property to lower computational costs: we calculate the mTree for $k=10$, then create 10000 rankings of length 10 and calculate $ \alphaadj $ under this setting.
%
%Then we set $ k=k+ $\texttt{stepsize}, and repeat the procedure until we reach the desired ranking length.
%
We can see in Figure~\ref{fig:why-adjustment-is-needed-multinomial} that $ \failprob $ grows very fast for small $k$, which makes an early adjustment of $\alpha$ most efficient to save computation time.
%
%Figure~\ref{fig:regression-time-saved} shows the reduction of computation time when we combine the binary search with a regression and the pre-adjustment strategy.

\subsection{Final Adjustment Algorithm}
Algorithm~\ref{alg:regression_search} shows the overall adjustment algorithm in pseudo-code, which performs the following steps:
%
\begin{enumerate}
	\item Define the necessary parameters: \texttt{kTarget}, \texttt{kStart}, \texttt{maxPreAdjustK}, and \texttt{num\_iterations}
	\item Adjust $\alpha$ for a mTree of size \texttt{kStart} to get $\alpha_{c_1}$ using binary search.
	\item Add pair $\left(\texttt{kStart}, \alpha_{c_1}\right)$ to a regression training set $R$.
	\item \label{stepBegin} Increase $\texttt{kStart}$ by $\texttt{stepsize}=\frac{\texttt{maxPreAdjustK}}{\texttt{num\_iterations}}$
	\item Compute a mTree with parameters $\texttt{kTarget}, p_G, \alpha_{c_1}$ and adjust $\alpha_{c_1}$.
	\item \label{stepEnd} Name result $\alpha_{c_2}$ and add pair $\left(\texttt{kStart}, \alpha_{c_2}\right)$ to $R$.
	\item Repeat steps (\ref{stepBegin}) -- (\ref{stepEnd}) until $\texttt{kStart} == \texttt{maxPreAdjustK}$.
	\item Train a regression model with training data $R$ to predict $\alpha_{c_r}$ for $\texttt{kTarget}$.
	\item Use binary search (Algorithm~\ref{alg:mult_binary}) to find $\alphaadj$ for parameters $k,p_G, \alpha_{c_r}$.
\end{enumerate}
%
\begin{algorithm}[t!]
	\caption{Algorithm \algoReg estimates the corrected significance level $\alphaadj$ such that the mTree $m(\alphaadj , k, p_G)$ has the probability of rejecting a fair ranking $\alpha$}
	\label{alg:regression_search} % But whenever possible refer to this algo. by name not number
	\small
	\AlgInput{\texttt{kStart} -- depth of the mTree to start with; $k$ -- the length of the ranking; $p_G$ -- the desired proportions of the protected groups; $\alpha$ -- the desired significance level;  \texttt{maxPreAdjustK} the maximum depth of the mTrees that are used as training data; \texttt{num\_iterations} -- the number of steps between \texttt{kStart} and \texttt{maxPreAdjustK}}
	\AlgOutput{$\alphaadj$ -- the adjusted significance}
	$R \leftarrow \lbrace \rbrace; \alpha_{\textit{new}} \leftarrow \alpha$	\\
	\AlgComment{divide the interval [$\text{kStart}, \text{maxPreAdjustK}$] into num\_iterations parts}
	$\texttt{stepsize} \leftarrow \max(\frac{\texttt{maxPreAdjustK}}{\texttt{num\_iterations}}, 1)$ \\
	\For{$i\leftarrow 0$ to $\texttt{num\_iterations}$}{
		\AlgComment{adjust $\alpha_{\text{new}}$ for the current kStart}
    	$\alpha_{\text{new}} \leftarrow \textsc{MultinomialBinarySearchAdjustment}(\alpha_{\textit{new}}, \texttt{kStart}, p_G)$ \\
    	\AlgComment{add the pair (kStart, $\alpha_{\textit{new}}$) to the training data $R$}
    	$R.\textit{put}(\texttt{kStart},\alpha_{\textit{new}})$ \\
    	\If{$\texttt{kStart} + \texttt{stepsize} \leq \texttt{maxPreAdjustK}$}{
    		$\texttt{kStart} \leftarrow \texttt{kStart} + \texttt{stepsize}$ \\
    	}\Else{
			break
    	}
    }
    $\texttt{coeffs} \leftarrow R.\textit{train}()$ \AlgComment{returns the vector of predicted coefficients for the curve over $R$}
    $\alpha_{c_r} \leftarrow \texttt{coeffs[0]} + \texttt{coeffs[1]} * k + \texttt{coeffs[2]} * k^2$ \\
    $\alphaadj \leftarrow \textsc{MultinomialBinarySearchAdjustment}(\alpha_{c_r},k,p_G)$ \\
    \Return{$\alphaadj$}
\end{algorithm}
\subsection{Further Optimizations}\label{subsec:further-optimizations}
In addition to the optimizations \emph{during} mTree calculation which we presented in Subsection~\ref{subsec:mtree-optimization}, our implementation caches already computed mTrees and their components to never do the same computation twice.
%
We remark that a mTree has to be computed only once for a particular combination of $k, p_G, \alpha$.

\subsubsection{MCDF Cache}\label{subsubsec:mcdf-cache}
Table~\ref{tbl:time-space} shows that the highest computational cost arises from computing the multinomial cumulative distribution function $F$.
%
In the worst case Algorithm~\ref{alg:imcdf} computes it $|G|+1$ times for each group $g$ in $m_g(i)$ and each position $i\leq k$.
%
However, the same calculation may be done many times:
%
As an example consider the (fictive) mTree nodes $(3, [2,1])$ and $(3, [1,2])$ at position $k=3$.
%
To compute the successors of node $(3, [2,1])$ we call Algorithm~\ref{alg:imcdf} with arguments $(4,[2,1])$, $(4,[3,1])$ and $(4,[2,2])$.
%
We store the results of these calculation in a map that we call MCDF cache with the algorithm arguments ($k$ and the minimum protected candidates of each group) as key and the corresponding mcdf as value.
%
Next we compute the successors of node $(3, [1,2])$ and call Algorithm~\ref{alg:imcdf} with arguments $(4,[1,2])$, $(4,[2,2])$ and $(4,[1,3])$.
%
We see that we would compute the MCDF for $(4,[2,2])$ twice, but instead we can now read it from the MCDF cache.
%
Note that the MCDF computation is only depends on $p_G$ and not on $alpha$.
%
We can therefore persist the MCDF cache on disk for a particular vector $p_G$ and load it for any mTree calculation with the same $p_G$ in the future.

\subsubsection{Stored mTrees}
\label{subsubsec:stored-mtrees}
During the computation of an adjusted mTree with parameters $k,p_G , \alpha$ we calculate many temporary mTrees (first the unadjusted ones, then the ones for the regression algorithm, then the ones for the binary search steps).
%
We persist all of the temporary mTrees plus the final tree in files for later usage.
%
The filenames contain the tree parameters, whether or not it is adjusted, and its probability to fail a fair ranking $\failprob$.
%
If any of these trees is needed at a later point in time it can be loaded from disc instead of being recomputed, be it as input for multinomial \algoFAIR or as temporary tree during a new adjusted mTree computation.

\subsection{Complexity Analysis}
\begin{table}[b!]
	\scalebox{0.75}{
		\begin{tabular}{lll}
			\toprule
			\textbf{Algorithm} & \textbf{Time Complexity} & \textbf{Space Complexity}\\
			\midrule
			\rowcolor[HTML]{C0C0C0}
			\algoImcdf & $\mathcal{O}(|G|) \cdot \mathcal{O}(\text{MCDF}(k,p,\alpha ))$ & $\mathcal{O}(|G|^2)$ \\
			\algoComputeMTree & $\mathcal{O}(|G|^{k}) \cdot \mathcal{O}(\text{\algoImcdf})$ & $\mathcal{O}(|G|^{k})$\\
			\rowcolor[HTML]{C0C0C0}
			\algoMultBinary & $\mathcal{O}(\log{}\frac{\alpha}{\epsilon}) \cdot (\mathcal{O}(k^2) + \mathcal{O}(\text{\algoComputeMTree}))$  & $\mathcal{O}(|G|^k)$\\
			\algoReg & $\mathcal{O}(\log{}\frac{\alpha}{\epsilon}) \cdot (\mathcal{O}(k^2) + \mathcal{O}(\text{\algoComputeMTree}))$ & $\mathcal{O}(|G|^k)$\\
			\rowcolor[HTML]{C0C0C0}
			\algoFAIR & $\mathcal{O}(n \log{} n) + \mathcal{O} (\text{\algoMultBinary}) + \mathcal{O}(k)$ & $\mathcal{O}(|G|^k + n + k)$ \\
			\bottomrule
		\end{tabular}
	}
	\caption{Time complexity for all algorithms without pre-computed results.\label{tbl:time-space}}
\end{table}

This section presents time and space complexity analyses for all algorithms from this section.
%
Table~\ref{tbl:time-space} shows the asymptotic costs for each algorithm without any pre-computed data (i.e. no mTree exists and the MCDF cache is empty).
%
The complexity analysis for \algoFAIR is done in Subsection~\ref{subsec:FAIR-complexity}, but Table~\ref{tbl:time-space} already contains the summary, such that it shows time and space complexity for all algorithms in this paper.

\subsubsection{\algoImcdf complexity}\label{subsubsec:imcdf-complexity}
The complexity of the multinomial CDF is dependent on the current position $i$ (also understood as \textit{number of trials}), $|G|$ the number of protected groups, and $m_{G,i}$ the vector of minimum required protected candidates for each group at position $i$ (also \textit{number of successes}).
%
We will write $\mathcal{O}(\text{MCDF}(i,p_G,\alpha))$ as the asymptotic complexity of this function.
%
In the library we used for our implementation, the time complexity to calculate the multinomial CDF at position $i$ is in $\mathcal{O}(\text{MCDF}(i,p_G,\alpha)) = \mathcal{O}(i^{|G|})$.
%
Thus, the most expensive position to calculate the multinomial CDF for is $k$.
%
For the overall time complexity of \algoImcdf we get $\mathcal{O}(|G|) \cdot \mathcal{O}(\text{MCDF}(k,p,\alpha ))$ with $|G|$ being the number of protected groups.
%
The space complexity is $\mathcal{O}(|G|^2)$ because we store $|G|$ nodes, with each node being an array of length $|G|$.
%
\subsubsection{\algoComputeMTree complexity}\label{subsubsec:mtree-complexity}
Algorithm \algoComputeMTree constructs an mTree of depth $k$.
%
There may be up to $|G|$ possible children for each node in the tree resulting in $|G|^{k-1} +1$ nodes, leading to a time complexity of $\mathcal{O}(|G|^{k}) \cdot \mathcal{O}(\text{\algoImcdf})$.
%
We store $|G|^{k-1} +1$ nodes with arrays of length $|G|$, leading to space complexity of $\mathcal{O}(|G|^{k})$.
%
\subsubsection{\algoMultBinary complexity}\label{subsubsec:multBinary-complexity}
Also, in the multinomial case we adjust $\alpha$ to $\alpha_c$ with a binary search heuristic.
%
However there is no discrete measure for mTrees as was in case of the mTable mass.
%
Therefore, the complexity of the binary search depends on the tolerance $\epsilon$ set beforehand as a stopping criteria.
%
As we are searching on the interval $\left[0,\alpha\right[$ the resulting number of possible mTrees is $\frac{\alpha}{\epsilon}$.
%
Thus the binary search needs $\mathcal{O}(\log{}\frac{\alpha}{\epsilon})$ time because we calculate one mTree and its failure probability for each step.
%
The failure probability is computed experimentally by creating 10.000 rankings and testing them against the current mTree.
%
This process needs $\mathcal{O}(10.000 \cdot k)$ to create the rankings plus $\mathcal{O}(k)$ to test each of them.
%
Hence the overall time complexity becomes $\mathcal{O}(\log{}\frac{\alpha}{\epsilon}) \cdot (\mathcal{O}(k^2) + \mathcal{O}(\text{\algoComputeMTree}))$.
%
The space complexity is in $\mathcal{O}(|G|^k)$ for storing three mTrees and their failure probability.
%
\subsubsection{\algoReg complexity}\label{subsubsec:regression-complexity}
Algorithm \algoReg has the same asymptotic complexity as \algoMultBinary.
%
However, a significant reduction in computation time compared can be achieved when combining the two, instead of using \algoMultBinary only (see Figure~\ref{fig:regression_adjustment_benefits}, green vs. orange line).
%
\subsection{Exact Solution for the Binomial Case}
In this section we described how to tackle the challenges of the inverse multinomial cumulative distribution function in the form of mTrees. However, the special case of two protected groups (the binomial case), which is extensively described by \citet{zehlike2017fair}, opens up a variety of more efficient and exact solutions. We developed an algorithm to compute the exact failure probability of a binomial mTree (mTable) and utilize it in a more efficient binary search for the correct significance level \cite{zehlike2020note}. The implementation can be found in our GitHub repository.\footnote{\url{https://github.com/MilkaLichtblau/Multinomial_FA-IR}}
